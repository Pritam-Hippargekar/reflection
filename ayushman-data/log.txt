Tracing with Spring Cloud Sleuth
# The trace-id is created when the request hits the very first service in the chain. For subsequent calls, an already existing trace-id is passed along, typically as an http header attribute.
# We do not need to write any custom code to create or propagate the trace contexts, as it is done automatically by Spring Cloud Sleuth. It typically intercepts the requests to do it. It also configures the logging context to include trace-id and other variables.
# Spring Cloud Sleuth can send trace information to Zipkin either synchronously over HTTP or asynchronously using a message broker such as "Kafka" or "RabbitMQ".
# Zipkin comes with native support for storing trace information either in memory or in a database such as Apache Cassandra, Elasticsearch, or MySQL.

$$ The scope of the SpanId is limited to service only.



<pattern>
    <pattern>{ "trace": "%mdc{X-B3-TraceId:-}" }</pattern>
</pattern>
<mdc>
    <excludeMdcKeyName>X-B3-SpanId</excludeMdcKeyName>
    <excludeMdcKeyName>X-B3-TraceId</excludeMdcKeyName>
    <excludeMdcKeyName>X-B3-ParentSpanId</excludeMdcKeyName>
    <excludeMdcKeyName>X-Span-Export</excludeMdcKeyName>
</mdc>



<pattern>
    <pattern>
        {
        "timestamp": "%d{\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\",UTC}",
         "timestamp": "%date{ISO8601}",
        "severity": "%level",
        "service": "${springAppName:-}",
        "service": "${APP_NAME:-}",
                                        "trace": "%X{trace_id:-}",
                                        "span": "%X{span_id:-}",
        "trace": "%X{X-B3-TraceId:-}",
        "span": "%X{X-B3-SpanId:-}",
        "exportable": "%X{X-Span-Export:-}",
         "parent": "%X{X-B3-ParentSpanId:-}",
        "pid": "${PID:-}",
        "pid": "%X{PID:-}",
        "thread": "%thread",
        "class": "%logger{40}",
        "class": "%logger{40}",
        "rest": "%message",
        "message": "%msg",
        "file":"%file",
            "line":"%line",
         "line_number": "%line",    
        "level":"%p",
        "level": "%level",
        "severity": "%level",
        "logger": "%logger",
        "host": "%property",
        "appname":"${springcloud_consume}",
        "serverName":"${server-name}"
        "stackTrace":"%ex",
        "stack_trace":"%xEx",
        "stack_trace": "%exception{full}"
        "ip": "%X{ip}",
        "url": "%X{url}",
        "method": "%method",
        "method": "%X{method}",
        "arg": "%X{arg}",
        "throwable": "%X{throwable}",
        }
    </pattern>
</pattern>

https://www.jianshu.com/p/576723ce2e3a

Each microservice logs through console and we’ll use the Docker fluentd logging driver to process these logs and send them to our elasticsearch.

Trace Id: A unique ID that remains the same throughout the request containing multiple microservices.
Span Id: A unique ID per microservice.
Spring Cloud Sleuth:
----------------------
It lets you track the progress of subsequent microservices by adding the appropriate headers to the HTTP requests.
The library is based on the MDC (Mapped Diagnostic Context) concept, where you can easily extract values put to context and display them in the logs.
If we inspect the http-headers in the request to XXXXController we'll see the following:
------------------------------------------------------------
"x-b3-traceid"="e40993b7256eda9d"
"x-b3-spanid"="0bcc87a9c8ea5fe3"
"x-b3-parentspanid"="71b4a062bf8f970c"


Distributed tracing platforms like OpenZipkin record trace data. 
Trace data is composed of a parent:child tree structure called a Directed Acyclic Graph (DAG for short). 
A root node represents the trace or overall journey, and each span represents an individual hop along the service route. 

Once you have Spring Cloud Sleuth on the classpath, it will automatically instrument common communication channels such as:
----------------------------------------------
Messaging like Kafka or RabbitMQ
HTTP headers via WebMVC and WebFlux controllers
Request headers made through WebClient, and RestTemplate


STEP 7: Configuring microservices to send logs to logstash
------------------------------------------------------------
Sending microservice logs to logstash requires the following dependencies to be added to each and every microservice.
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>5.3</version>
</dependency>
<dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-core</artifactId>
    <version>1.2.3</version>
</dependency>
The next configuration to add is to create a file called logback.xml in resource folder of every microservice with the following contents:
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
    <include resource="org/springframework/boot/logging/logback/base.xml"/>
    <appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5044</destination>
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <mdc/>
                <context/>
                <version/>
                <logLevel/>
                <loggerName/>
                <message/>
                <pattern>
                    <pattern>
                        {
                            "serviceName": "account-service"
                        }
                    </pattern>
                </pattern>
                <threadName/>
                <stackTrace/>
            </providers>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="logstash"/>
    </root>
    <logger name="org.springframework" level="INFO"/>
    <logger name="com.cinema" level="INFO"/>
</configuration>
The steps outlined above if followed diligently will enable you to put in a place distributed tracing in your micro services architectures and be able to visualise your logs through kibana and search through them using elasticsearch.



logback-spring.xml
=================
<? xml version = "1.0" encoding = "UTF-8" ?> 
<configuration> 
    <include resource = "org/springframework/boot/logging/logback/defaults.xml" />

    <springProperty scope = "context" name = "springAppName" source = "spring.application.name" /> 
    <property name = "CONSOLE_LOG_PATTERN" value = "%clr(%d{yyyy-MM-dd HH:mm:ss. SSS}){faint} %clr(${level:-%5p}) %clr([${springAppName:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId: -},%X{X-Span-Export:-}]){yellow} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t] ){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}" / >  
    
    <appender name = "CONSOLE" class = "ch.qos.logback.core.ConsoleAppender" > 
       <encoder> <pattern> ${CONSOLE_LOG_PATTERN} </pattern> <charset> UTF-8 </charset> </encoder> 
    < /appender> 
        
    <root level = "INFO" > 
        <appender-ref ref = "CONSOLE" /> 
    </root> 
</configuration>








1. The first springboot-logstash environment construction
1.1 Add maven
        <dependency>
            <groupId>net.logstash.logback</groupId>
            <artifactId>logstash-logback-encoder</artifactId>
            <version>6.3</version>
        </dependency>
1.2 logback-spring.xml adds log printing
    <!--LOGSTASH config -->
    <appender name="LOGSTASH"
        class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5000</destination>
        <!--<encoder charset="UTF-8"
            class="net.logstash.logback.encoder.LogstashEncoder"> -->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}
                [service:${springAppName:-}]
                [traceId:%X{X-B3-TraceId:-},spanId:%X{X-B3-SpanId:-},parentSpanId:%X{X-B3-ParentSpanId:-},exportable:%X{X- Span-Export:-}]
                [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- Set the character set here -->
        </encoder>
    </appender>
1.3 configure logstash
input {
  tcp {
    port => 5000
  }
}
filter {
  grok {
    match => {
    "message" => "%{TIMESTAMP_ISO8601:logTime} %{GREEDYDATA:service} %{GREEDYDATA:thread} %{LOGLEVEL:level} %{GREEDYDATA:loggerClass}-%{GREEDYDATA:logContent}"}
  }
}
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "springboot-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "changeme"
  }
}








- Spans are timed operations representing small logical units of work. 
  Each span contains some basic information like Start Time, Duration, etc. 
  We can think of Spans as the building blocks of a distribute trace.
    Spans could be
      Request from Loadbalance to Order Service
      Request Order Service to Product Service
      Product Service to Database
      Order Service to Database
- Trace is nothing but a complete execution path of a request and can be thought of as a collection of Directed Acyclic Graph of Spans.
  (Trace is the entire HTTP Request/Response)



What you are seeing there, are the respective parameters:[food-order-publisher,888114b702f9c3aa,888114b702f9c3aa,true]
-------------------------------------------------------------
appname – the name of the application that logged the span
traceId – the id of the latency graph that contains the span
spanId – the id of a specific operation
exportable – whether the log should be exported to Zipkin or not (more about Zipkin later)



Zipkin Client
==================
Zipkin client contains Sampler which collects data from ms apps with the help of sleuth and provides it the zipkin server.
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>

spring.zipkin.service.name=food-order-consumer
spring.zipkin.sender.type=web             # we want to report data to Zipkin via HTTP calls rather than a message queue  
spring.zipkin.baseUrl=http://localhost:9411
spring.sleuth.sampler.percentage=1.0      # Default is 0.1 which means 10%

@Bean
fun sleuthTraceSampler(): Sampler {
    return Sampler.ALWAYS_SAMPLE
}

Spring Boot App → Log File → Logstash → Elasticsearch. 
There are other ways of accomplishing the same thing, such as configuring logback to use TCP appender to send logs to a remote Logstash instance via TCP, 
and many other configurations.
event will either be a single log line or multiline log event grouped according to the rules described

try {
  parseXML();
} catch (Exception e) {
    // documentId is a StructuredArgument for structured logging, e is used for stacktrace
    logger.error("Parsing xml document failed!", kv("documentId", documentId), e);
    logger.error("Parsing xml document {} failed, underlying cause was [{}]", documentId, e.getMessage(), e);
}





input {
  file {
    path => "/var/app/current/logs/javaApp.log"     ## It must be absolute
    mode => "tail"                           ## only read new entries at the end of the file
    type => "java"                           ## it's just additional piece of metadata in case you will use multiple types of log files in the future.
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601} "    ## Each new log event needs to start with date
      negate => true                         ## if it doesn't start with a date
      what => "previous"                     ## then it should be grouped with a previous line
    }
  }
}
filter {
  #If log line contains tab character followed by 'at' then we will tag that entry as stacktrace
  if [message] =~ "\tat" {
    grok {
      match => ["message", "^(\tat)"]
      add_tag => ["stacktrace"]
    }
  }

  #Grokking Spring Boot's default log format
  grok {
    match => [ "message", 
               "(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- \[(?<thread>[A-Za-z0-9-]+)\] [A-Za-z0-9.]*\.(?<class>[A-Za-z0-9#_]+)\s*:\s+(?<logmessage>.*)",
               "message",
               "(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- .+? :\s+(?<logmessage>.*)"
             ]
  }

  #Parsing out timestamps which are in timestamp field thanks to previous grok section
  date {
    match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss.SSS" ]
  }
}









input {
        file {
        path => "/root/mult.log"
        start_position => "beginning"
        sincedb_path => "/dev/null"
        codec => multiline{
                 pattern => "^ -%{SPACE}%{TIMESTAMP_ISO8601}"
                 negate => true
                 what => "previous"
        }
}
 }
filter {
    grok {
     match => [
       "message", "(?m)^ -%{SPACE}%{TIMESTAMP_ISO8601:time} \[%{WORD:main}\] %{LOGLEVEL:loglevel}%{SPACE}\(%{JAVACLASS:class}\) %{DATA:mydata}\n(\t)?%{GREEDYDATA:stack}",
       "message", "^ -%{SPACE}%{TIMESTAMP_ISO8601:time} \[%{WORD:main}\] %{LOGLEVEL:loglevel}%{SPACE}\(%{JAVACLASS:class}\) %{GREEDYDATA:mydata}" ]
        break_on_match => false
 }
    date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z"]
 }
}

output {
  stdout { codec => rubydebug}
elasticsearch {
    host => "localhost"
  }
}








output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "인덱스명지정"
        user => "아이디입력"
        password => "비밀번호입력"
    }
}









input {
  file {
    path => "/usr/logstash/logstash-7.2.0/ml-latest-small/movies.csv" #Pay attention to change to your own directory
    start_position => "beginning" 
    sincedb_path => "/dev/null"
  }
}
filter {
  csv {
    separator => "," 
    columns => ["id","content","genre" ]
  }
  mutate {
    split => { "genre" => "|" }
    remove_field => ["path", "host","@timestamp","message" ]
  }
  mutate {

    split => ["content", "(" ]
    add_field => { "title" => "%{[content][0]}" }
    add_field => { "year" => "%{[content][2]}" }
  }
  mutate {
    convert => {
       "year" => "integer"
    }
    strip => ["title" ]
    remove_field => ["path", "host","@timestamp","message","content" ]
  }
}
output {
   elasticsearch {
     hosts => http: // 192.168.1.204:9200 #Pay attention to modify to your own ES 
     index => "movies" 
     document_id => "%{id}"
   }
  stdout{}
}


<?xml version="1.0" encoding="UTF-8"?>
<!--The log saves log information with different log levels into different files-->
<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml" />

    <!--springProperty: Find the corresponding configuration item in the properties/yml file -->
    <springProperty scope="context" name="springAppName" source="spring.application.name" />
    <springProperty scope="context" name="logFilePath" source="logging.config.path" />

    <!-- The output location of the log in the project -->
    <property name="LOG_FILE" value="${BUILD_FOLDER:-build}/${springAppName}" />

    <!-- console log output style -->
    <property name="CONSOLE_LOG_PATTERN" 
              value ="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${ PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}" />

    <!-- console output appender-->
    <appender name="console" class ="ch.qos.logback.core.ConsoleAppender">
        <filter class ="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <!-- Log output encoding -->
        <encoder class ="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <!-- Appender for logstash output in JSON format -->
    <appender name="logstash"
              class ="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>192.168.1.204:9665</destination>
        <!-- Log output encoding -->
        <encoder class ="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "severity": "%level" ,
                         "service": "${springAppName:-}" ,
                         "trace": "%X{X-B3-TraceId:-}" ,
                         "span": "%X{X- B3-SpanId:-}" ,
                         "exportable": "%X{X-Span-Export:-}" ,
                         "pid": "${PID:-}" ,
                         "thread": "%thread" ,
                         "class ": "%logger{40}" ,
                         "rest": "%message"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
    </appender>

    <!--File format output appender-->
    <appender name="file" class ="ch.qos.logback.core.rolling.RollingFileAppender">
        <!--Define the path of log output-->
        <!--The scheduler.manager.server.home here is not set in the above configuration, so the value configured when java starts will be used-->
        <!--For example, configure this property through java -Dscheduler.manager.server.home=/path/to XXXX -->
        <file>${logging.path}/spring-boot/elk.log</file>
        <!--Define the log rolling strategy-->
        <rollingPolicy class ="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--The format of the file name when defining the file scrolling -->
            <fileNamePattern>${scheduler.manager.server.home}/logs/${app.name}.%d{yyyy-MM- dd.HH}.log
             </fileNamePattern>
            <!--60-day time period, the maximum log volume is 20GB-->
            <maxHistory>60</maxHistory>
            <!-- This attribute is only supported after version 1.1.6 -->
            <totalSizeCap>20GB</totalSizeCap>
        </rollingPolicy>
        <triggeringPolicy class ="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <!--The maximum size of each log file is 100MB-->
            <maxFileSize>100MB</maxFileSize>
        </triggeringPolicy>
        <!--Define output format-->
        <encoder>
            <pattern>%d [%thread] %-5level %logger{36} [%file : %line] - %msg%n</pattern>
        </encoder>
    </appender>

    <!--logger is used to set the log printing level of a certain package or a specific class and specify appender-->
    <!--The logger can be obtained through LoggerFactory.getLogger("mytest")-->
    <!--Since this logger automatically inherits the root appender, there is already a stdout appender in the root, and I have introduced the stdout appender-->
    <!--If additivity="false" is not set, a log will be output twice in the console, and the division of labor is done through appender- ref, and root is responsible for console and logstash
      This logger is responsible for file output -->
    <!--additivity indicates whether to use the appender configured by rootLogger for output -->
    <logger name="test" level="INFO" additivity="false">
        <appender-ref ref="file"/>
    </logger>

    <!-- The root logger is also a kind of logger, and has only one level attribute -->
    <root level="INFO">
        <appender-ref ref="console" />
        <appender-ref ref="logstash" />
    </root>

</configuration>






<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml" />
    <appender name="LOGSTASH" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>./infrastructure/elk-docker-compose/logs/spring-boot-logs.log</file> <!-- add your log file path -->.
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>./logs/spring-boot-logs-%d{yyyy-MM-dd}.log</fileNamePattern>
            <maxHistory>1</maxHistory><!-- log file will override after one day-->
        </rollingPolicy>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder" />
    </appender>
    <root level="INFO">
        <appender-ref ref="LOGSTASH" />
    </root>
</configuration>



https://insight-bgh.tistory.com/519
-----------------------------------------

1. Add logstash dependency (build.gradle )
implementation 'net.logstash.logback:logstash-logback-encoder:6.6'
 

2. Add logback-spring.xml file
Add logback-spring.xml under resources folder.

<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="30 seconds">

    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%-5level %d{HH:mm:ss.SSS} [%thread] %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}[%-5level] : %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="ROLLING" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>mylog.txt</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- rollover daily -->
            <fileNamePattern>mylog-%d{yyyy-MM-dd}.%i.txt</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy
                    class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- or whenever the file size reaches 100MB -->
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>[%-5level] %d{HH:mm:ss.SSS} %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- Log Stash 사용시 사용할 TCP 통신 정보 -->
    <appender name="stash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>서버ip정보:logstash포트</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"></encoder>
    </appender>

   
    <root level="INFO">
        <appender-ref ref="console" />
        <appender-ref ref="stash" />
    </root>

</configuration>
 

3. Add logback-spring.xml to application.yml
logging:
  config: classpath:logback-spring.xml
 

4. Add spring-cloud-starter-sleuth (build.gradle )
If you want to see logs by traceId and spanId, add spring-cloud-starter-sleuth dependency. It is often used to track logs in the msa environment.
implementation group: 'org.springframework.cloud', name: 'spring-cloud-starter-sleuth', version: '3.0.4'






















# spring.application.name and server.port are set in the main methods,
# so not done here
#logging.level.org.springframework.web=DEBUG
spring.sleuth.traceId128=true
spring.zipkin.base-url=http://127.0.0.1:9411/
spring.sleuth.sampler.probability=1.0
# Adds trace and span IDs to logs (when a trace is in progress)
#logging.pattern.level=[%X{traceId}/%X{spanId}] %-5p [%t] %C{2} - %m%n
# Propagates a field named 'user_name' downstream
spring.sleuth.baggage.remote-fields=user_name
spring.application.name=sale
server.port=8081

Logstash
========
Logstash is not just an input | filter | output data flow, 
but an input | decode | filter | encode | output data flow! codec is used to decode and encode events.


Execute the command to check whether the file configuration is OK
------------------------------------------------------logstash -e "input { stdin { } } output { stdout {} }"
logstash -f first-pipeline.conf --config.test_and_exit

Execute the command to run logstash
-------------------------------------
logstash -f first-pipeline.conf --config.reload.automatic

--config.reload.automatic option allows you to automatically enable configuration reloading without restarting Logstash





input {
    file {
        path => ["/var/log/**/*.log", "/var/log/message"] #绝对路径
        type => "system"
        start_position => "beginning"
        sincedb_path => /dev/null  #每次重启自动从头开始读
    }
}
---
input {
    stdin {
        codec => multiline {
            pattern => "^\["
            negate => true
            what => "previous"
        }
    }
}


https://elasticsearch.tutorials24x7.com/blog/how-to-install-elasticsearch-kibana-and-logstash-elk-elastic-stack-on-windows


https://blog.frankel.ch/structuring-data-with-logstash/
Indexing unstructured log messages is not very useful. Logstash configuration allows to pre-parse unstructured data and send structured data instead.
Grok
Grokking data is the usual way to structure data with pattern matching.

Dissect
The Grok filter gets the job done. But it seems to suffer from performance issues, especially if the pattern doesn’t match. 
An alternative is to use the dissect filter instead, which is based on separators.
it’s much easier to write a separator-based filter than a regex-based one.

filter {
  dissect {
	mapping => { "message" => ... }
  }
  mutate {
    strip => [ "log", "class" ]                        strip additional spaces with Logstash
  }
}





Log file configuration
-----bin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'----------------------------------------------------------------------------logstash -e 'input { stdin { } } output { stdout {} }'
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml" />
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy
                class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <FileNamePattern>${user.dir}/logs/user-%d{yyyyMMdd}.log
            </FileNamePattern>
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <triggeringPolicy
                class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <MaxFileSize>10MB</MaxFileSize>
        </triggeringPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>
                {"level":"%level","pid":${PID},"thread":"%thread","requestId":"%X{requestId}","remoteIp":"%X{remoteIp}","traceId":"%X{X-B3-TraceId:-}","spanId":"%X{X-B3-SpanId:-}","parentSpanId":"%X{X-B3-ParentSpanId:-}","class":"%logger{50}","method":"%method","line":%line,"message":"%message","stack_trace":"%exception{10}"}%n
            </pattern>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>






























input {
  file{
    path => "C:/temp/*.logs"
    start_position => "beginning"
    sincedb_path => "NUL"
    codec => json
  }
}
filter {
  mutate {
    add_field => {"source" => "Medium"}
  }
}
output {
 file {
   path => "c:/temp/logstash_out.log"
 }
}










input {
    file {
        path => ["/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/redis.log"]
        start_position => "beginning"
        sincedb_path => "/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/sincedb.db"
        type => "redis"
	      mode => "read"
        stat_interval => "1 second"
        discover_interval => 15
        sincedb_write_interval => 15
        add_field => {
            "custom_mode" => "tail"
        }
    }

    file {
        path => ["/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/springboot.log"]
        start_position => "end"
        sincedb_path => "/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/sincedb.db"
	      mode => "tail"
        type => "springboot"
    }
}



  stdout {
    codec =>  rubydebug {
      metadata => true
    }
  }

stdout {
    codec => line {
        charset => "UTF-8"
    }
  }


input {
  file {
    path => "/var/log/messages"
    tags => ["sys", "logstash_messages", "%{host}" ]
    type => syslog
  }
  file {
    path => "/var/log/cron"
    tags => ["sys", "logstash_cron", "%{host}" ]
    type => syslog
  }
  file {
    path => "/var/log/secure"
    tags => ["sys", "logstash_secure", "%{host}" ]
    type => syslog
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
output {
  if [tags][1] == "logstash_messages" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "%{tags[0]}.%{tags[1]}-%{+YYYY.MM.dd}"
    }
    file {
      path => "/var/log/logstash/%{tags[0]}/%{tags[1]}.log"
    }
  } else if [tags][1] == "logstash_cron" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "%{tags[0]}.%{tags[1]}-%{+YYYY.MM.dd}"
    }
    file {
      path => "/var/log/logstash/others/%{tags[1]}.log"
    }
  } else {
    file {
      path => "/var/log/logstash/others/other.log"
    }
  }
}



# logstash/pipeline/logstash.conf
input {
    beats {
        port => 5044
    }
}
filter {
    grok {
        match => {
            "message" => "%{JAVACLASS:exception}:\s%{GREEDYDATA:stacktrace}"
        }
        add_tag => ["stacktrace"]
    }
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns"]
        match => {
            "message" => "%{TIMESTAMP_ISO8601:log_timestamp}\s*%{LOGLEVEL:log_level}\s*%{POSINT:logged_for_pid}\s*--- \[+%{NOTSPACE:logged_for_thread}+\]\s*%{JAVACLASS:logger}%{GREEDYDATA:loggercd}\s*%{MSG:log_message}"
        }
        add_tag => ["spring_boot_log"]
    }
    if [loggercd] {
        mutate {
            replace => { "logger" => "%{logger}%{loggercd}" }
            strip => ["logger"]
            remove_field => ["loggercd"]
        }
    }
    if "stacktrace" in [tags] or "spring_boot_log" in [tags] {
        mutate {
            remove_tag => ["_grokparsefailure"]
        }
    }
}
output {
    elasticsearch {
        hosts => ["${ELASTIC_HOSTS}"]
        user => "${ELASTIC_USER}"
        password => "${ELASTIC_PASSWORD}"
        index => "spring-boot-app-logs-%{+YYYY.MM.dd}"
    }
}


1) These specify the logging level (INFO)                     https://logback.qos.ch/manual/layouts.html#conversionWord
2) the appenders (where to log)  (Console)                     https://logback.qos.ch/manual/layouts.html#conversionWord
3) the format of the log messages.
%clr(%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}}){faint}
%clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint}
%clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint}
%m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}


%clr specifies a colour.
%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}} is the current date, ${VARIABLE}:- we should use the $VARIABLE environment variable for the format, if it is available, and if not, fall back to default.
${LOG_LEVEL_PATTERN:-%5p} will print the log level with right padding up to 5 characters (E.g “INFO” becomes “INFO “ but “TRACE” will not have the trailing space).
${PID:- } The environment variable $PID, if it exists. If not, space.
t	The name of the thread triggering the log message.
logger	The name of the logger (up to 39 characters), in our case this is the class name.
%m	The log message.
%n	The platform-specific line separator.
%wEx	If one exists, wEx is the stack trace of any exception, formatted using Spring Boot’s ExtendedWhitespaceThrowableProxyConverter.
%msg - outputs the actual log message.
%n - line break
%M - outputs the name of the method that the log message occurred in
%logger{36} The number inside the brackets represents the maximum length of the package + class name. If the output is longer than the specified length it will take a substring of the first character of each individual package starting from the root package until the output is below the maximum length.
%ex{short/short/any-number}






<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>
        %d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger{36}.%M - %msg%n
      </pattern>
    </encoder>
  </appender>

  <root level="info">
    <appender-ref ref="STDOUT"/>
  </root>
</configuration>
logging.level.root=info
logging.pattern.console=%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n

Package level logging can also be defined by simply using the package name instead of the class name in the logger tag.
logging.level.com.lankydan.service=debug
<logger name="com.lankydan.service" additivity="false" level="debug">
  <appender-ref ref="STDOUT" />
</logger>









<?xml version="1.0" encoding="UTF-8"?>
<!--
Default logback configuration provided for import
-->
<included>
	<conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter" />
	<conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter" />
	<conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter" />
	<property name="CONSOLE_LOG_PATTERN" value="${CONSOLE_LOG_PATTERN:-%clr(%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>
	<property name="FILE_LOG_PATTERN" value="${FILE_LOG_PATTERN:-%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}} ${LOG_LEVEL_PATTERN:-%5p} ${PID:- } --- [%t] %-40.40logger{39} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

	<logger name="org.apache.catalina.startup.DigesterFactory" level="ERROR"/>
	<logger name="org.apache.catalina.util.LifecycleBase" level="ERROR"/>
	<logger name="org.apache.coyote.http11.Http11NioProtocol" level="WARN"/>
	<logger name="org.apache.sshd.common.util.SecurityUtils" level="WARN"/>
	<logger name="org.apache.tomcat.util.net.NioSelectorPool" level="WARN"/>
	<logger name="org.eclipse.jetty.util.component.AbstractLifeCycle" level="ERROR"/>
	<logger name="org.hibernate.validator.internal.util.Version" level="WARN"/>
	<logger name="org.springframework.boot.actuate.endpoint.jmx" level="WARN"/>
</included>


(Logback is a logging framework for Java, and a successor to the old log4j.)
How to customise Spring Boot’s logging configuration
You can customise the default logging configuration in one of these ways:
1) Setting some Spring Boot logging properties – which can go in your application.properties, application.yml, or as environment variables in your shell
    # Examples for Spring Boot 2.x
    logging.file.path=.             # write logs to the current directory
    logging.file.path=/home/logs    # write logs to /home/logs
    logging.file.path=/mnt/logdir   # write logs to /mnt/logdir
    # Example for Spring Boot 2.x
    logging.file.name=myapp.log

    Just convert each property to upper-case, and change dots to underscores.
2) Adding a logback.xml onto the classpath, which Spring Boot will detect, and use to configure Logback
3) Adding a logback-spring.xml onto the classpath, which Spring Boot will detect, and use to configure Logback





How the magic works
How does Spring detect Logback and abstract it away from us?

1) Looking around the source code, I found these relevant clues (hunted down in Spring Boot v2.6.6):
2) Logback should appear on the classpath in most Spring Boot projects because… spring-boot-starter-web includes the dependency spring-boot-starter-logging, which includes logback as a dependency.
3) Spring’s logging system detects Logback because… The abstract class LoggingSystem contains some code which detects and returns the logging system that’s currently in use. It checks which classes are present in the classloader.
4) Spring knows how to write logs with Logback because… There’s a subclass of LoggingSystem, called LogbackLoggingSystem, which basically configures and works with Logback.
We don’t need to configure Logback ourselves because… Inside DefaultLogbackConfiguration, Spring Boot does some programmatic auto-configuration of Logback for us, so we don’t need to configure it manually.

Push log entries from a Spring Boot application
By default, Spring Boot uses Logback for logging which can be fine-tuned by creating a logback-spring.xml configuration:

<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
    <include resource="org/springframework/boot/logging/logback/console-appender.xml"/>

    <appender name="logstashNetworkAppender" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5000</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
        </encoder>
        <keepAliveDuration>5 minutes</keepAliveDuration>
    </appender>

    <root level="INFO">
        <appender-ref ref="logstashNetworkAppender"/>
        <appender-ref ref="CONSOLE"/>
    </root>
</configuration>
This snippet adds the LogstashTcpSocketAppender which sends the log entries in JSON format to the Logstash node and it also keeps using the default Console Appender.

application-name is the name of the application in the application.properties or application.yml/yaml file.
- zipkin-export indicates whether the logs can be exported to Zipkin server or not.
- The default value is false and if you want to export it to Zipkin server you need to make it true.
- Sleuth helps not only to track timing, but also to catch errors so that they can be analyzed or correlated with logs.
[application-name,trace-id,span-id,zipkin-export]
- The traceId and spanId in the trace information of Spring Cloud Sleuth will be automatically added to the MDC of Slf4j.
- The default log framework of spring is logback.
EX:-
@GetMapping("/hello")
public String hello(@RequestParam String name, @RequestHeader Map<String, String> headers) {
    log.info("headers:{}", headers);
}// headers:{x-b3-traceid=6dd57dc2ad55c58f, x-b3-spanid=b5e6de658b261ac7, x-b3-parentspanid=6dd57dc2ad55c58f, x-b3-sampled=1, accept=*/*, user-agent=Java/1.8.0_202, host=localhost:8081, connection=keep-alive}

logging:
  level:
    root: INFO
    com.abc.demo: DEBUG
  pattern:
    console: "%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%8.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %X{REQUEST_ID} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"

"%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"
2020-04-23 21:09:44.107 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP GET "/posts" , parameters={requestId=1234}, body=, remote_address=0:0:0:0:0:0:0:1
2020-04-23 21:09:44.110 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP RESPONSE [{"id":1,"title":"Spring Boot","body":"All about Spring boot microservice"},{"id":2,"title":"Java","body":"Learn Streams in Java"},{"id":3,"title":"JavaScript","body":"Whats new in ES6"}]



tags => [ “technical”, “log”]
mutate {
        add_field => { "instance_name" => "%{app_name}-%{host}:%{app_port}" }
    }


# build the project
mvn clean install -DskipTests

# Start author services
java -jar target/author-services-0.0.1-SNAPSHOT.jar

Around Advice  - It wraps actual method call and provides interception point for both before and after execution of this method
Before Advice - It provides interception point before method call
After Advice - It provides interception point after method call
After Throwing Advice - It provides interception point after method call only if an exception is thrown



<dependency>
  <groupId>net.logstash.logback</groupId>
  <artifactId>logstash-logback-encoder</artifactId>
  <version>6.3</version>
</dependency>
Create a file called logback-spring.xml in ./src/main/resources:

<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE configuration>
<configuration scan="true">
    <include resource="org/springframework/boot/logging/logback/base.xml"/>
    <springProperty scope="context" name="app_name" source="spring.application.name"/>
    <springProperty scope="context" name="app_port" source="server.port"/>
    <springProperty scope="local" name="logstash_host" source="logstash.host"/>
    <springProperty scope="local" name="logstash_port" source="logstash.port"/>
    <appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <param name="Encoding" value="UTF-8"/>
        <remoteHost>${logstash_host}</remoteHost>
        <port>${logstash_port}</port>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"/>
    </appender>
    <root level="INFO">
        <appender-ref ref="logstash"/>
    </root>
</configuration>
The above uses spring.application.name, server.port as well as two custom variables: logstash.host and logstash.port to configure the appender, so make sure they exist in your application.properties file:

server.port=8080
spring.application.name=spring-boot-kibana-demo
logstash.host=localhost
logstash.port=5000
























<?xml version="1.0" encoding="UTF-8"?>
<configuration>

    <!-- Get the property for app name from the properties file -->
    <springProperty scope="context" name="appName" source="spring.application.name"/>

    <!-- Configuration when the profile is staging or prod -->
    <springProfile name="staging,prod">
        <appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
            <file>logs/${appName}.log</file>
            <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
                <!-- daily rollover -->
                <fileNamePattern>logs/%d{yyyy-MM-dd,aux}/${appName}.%d{yyyy-MM-dd}.%i.log</fileNamePattern>
                <!-- keep 30 days' worth of history -->
                <maxHistory>30</maxHistory>
                <timeBasedFileNamingAndTriggeringPolicy
                        class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                    <!-- or whenever the file size reaches 80MB -->
                    <maxFileSize>80MB</maxFileSize>
                </timeBasedFileNamingAndTriggeringPolicy>
            </rollingPolicy>
            <encoder>
                <pattern>%d %5p | %t | %-54logger{55} | %m %n</pattern>
            </encoder>
        </appender>

        <logger name="${appName}-logger">
            <level value="INFO"/>
        </logger>

        <root>
            <level value="INFO"/>
            <appender-ref ref="FILE"/>
        </root>
    </springProfile>

    <!-- Configuration when the profile is dev -->
    <springProfile name="dev">
        <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
            <encoder>
                <pattern>%d %5p | %t | %-55logger{55} | %m %n</pattern>
            </encoder>
        </appender><springProperty scope="context" name="appName" source="spring.application.name"/>
        <logger name="${appName}-logger">
            <level value="INFO"/>
        </logger>
        <root>
            <level value="INFO"/>
            <appender-ref ref="CONSOLE"/>
        </root>
    </springProfile>
</configuration>






















<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <springProperty scope="context" name="logfiledirectory" source="logging.directory" defaultValue="/var/log"/> 
    <springProperty scope="context" name="logfile" source="logging.spring-customer-service.file" defaultValue="spring-customer-service.log"/> 
    <springProperty scope="context" name="logstashhost" source="logging.logstash.host" defaultValue="localhost:5043"/>
    <springProperty scope="context" name="maxFileSize" source="logging.filesize.max" defaultValue="100MB"/>
    <springProperty scope="context" name="maxArchiveSize" source="logging.archivesize.max" defaultValue="10GB"/>
    <springProperty scope="context" name="maxHistory" source="logging.history.max" defaultValue="60"/>
    <springProperty scope="context" name="devLogLevel" source="logging.dev.loglevel" defaultValue="DEBUG"/>
    <springProperty scope="context" name="stagingLogLevel" source="logging.staging.loglevel" defaultValue="DEBUG"/>
    <springProperty scope="context" name="productionLogLevel" source="logging.production.loglevel" defaultValue="INFO"/>
        <appender name="logStashAppender"
			class="net.logstash.logback.appender.LogstashTcpSocketAppender">
			<destination>${logstashhost}</destination>
			<!-- encoder is required -->
			<encoder class="net.logstash.logback.encoder.LogstashEncoder" />
		</appender>
	
		<appender name="consoleOut" class="ch.qos.logback.core.ConsoleAppender">
			<!-- encoders are assigned the type ch.qos.logback.classic.encoder.PatternLayoutEncoder 
				by default -->
			<encoder>
				<pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
			</encoder>
		</appender>
		
		<appender name="rolling"
			class="ch.qos.logback.core.rolling.RollingFileAppender">
			<file>${logfiledirectory}/${logfile}</file>
			<rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy">
		      <!-- rollover daily -->
		      <fileNamePattern>${logfile}-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
		       <!-- each file should be at most 100MB, keep 60 days worth of history, but at most 20GB -->
		       <maxFileSize>${maxFileSize}</maxFileSize>    
		       <maxHistory>${maxHistory}</maxHistory>
		       <totalSizeCap>${maxArchiveSize}</totalSizeCap>
		    </rollingPolicy>
			<!-- <encoder class="net.logstash.logback.encoder.LogstashEncoder" /> -->
			<encoder>
				<pattern>%d{dd-MMM-yy HH:mm:ss.SSS} %-5level [%thread] %logger{10} [%file:%line] %msg%n</pattern>
			</encoder>
		</appender>

	<springProfile name="dev"> 
		<root level="${devLogLevel}">
			<appender-ref ref="logStashAppender" />
		</root>      
		<logger name="org" level="${devLogLevel}">
			<appender-ref ref="consoleOut" />
		</logger>
		<logger name="org.appfabs.sample.logaggregation" level="${devLogLevel}">
			<appender-ref ref="rolling" />
		</logger>
	</springProfile>
	
	<springProfile name="staging"> 
		<root level="${stagingLogLevel}">
			<appender-ref ref="logStashAppender" />
		</root>      
		<logger name="org" level="${stagingLogLevel}">
			<appender-ref ref="consoleOut" />
		</logger>
		<logger name="org.appfabs.sample.logaggregation" level="${stagingLogLevel}">
			<appender-ref ref="rolling" />
		</logger>
	</springProfile>
	
	<springProfile name="production"> 
		<root level="${productionLogLevel}">
			<appender-ref ref="logStashAppender" />
		</root>      
		<logger name="org" level="${productionLogLevel}">
			<appender-ref ref="consoleOut" />
		</logger>
		<logger name="org.appfabs.sample.logaggregation" level="${productionLogLevel}">
			<appender-ref ref="rolling" />
		</logger>
	</springProfile>
</configuration>



===========================================================================================
 <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
      <p>
        <timestamp>
          <timeZone>UTC</timeZone>
        </timestamp>
        <p>
          <p>
            {
            "severity": "%level",
            "service": "${springAppName:-}",
            "trace": "%X{X-B3-TraceId:-}",
            "span": "%X{X-B3-SpanId:-}",
            "parent": "%X{X-B3-ParentSpanId:-}",
            "exportable": "%X{X-Span-Export:-}",
            "pid": "${PID:-}",
            "thread": "%thread",
            "pvai.userId": "%X{context.userId:-}",
            "pvai.moduleId": "%X{context.moduleId:-}",
            "pvai.caseId": "%X{context.caseId:-}",
            "class": "%logger{40}",
            "rest": "%message"
            }
          </pattern>
        </pattern>
      </providers>
    </encoder>


filter {
  if [message] =~ "timestamp" {
    grok {
      match => ["message", "^(timestamp)"]
      add_tag => ["stacktrace"]
    }
  }
}


    <!-- logback-spring.xml -->
    <configuration>
        <springProfile name="dev">
            <include resource="org/springframework/boot/logging/logback/base.xml"/>
        </springProfile>
        <springProfile name="test">
            <include resource="org/springframework/boot/logging/logback/base.xml"/>
        </springProfile>

        <springProfile name="default">
            <appender name="jsonConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
                <encoder class="net.logstash.logback.encoder.LogstashEncoder">
                    <includeMdc>true</includeMdc>
                </encoder>
            </appender>
            <root level="INFO">
                <appender-ref ref="jsonConsoleAppender"/>
            </root>
        </springProfile>
    </configuration>


    <!-- logback-spring.xml -->
    <configuration>

        .. dev and test as above ...

        <springProfile name="default">
            <appender name="jsonConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
                <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
                    <providers>
                        <timestamp/>
                        <version/>
                        <message/>
                        <loggerName/>
                        <threadName/>
                        <logLevel/>
                        <mdc/>
                        <!-- repeats log arguments as root json fields -->
                        <arguments/>
                        <pattern>
                            <pattern>
                                {
                                  "myField": {
                                    "mySubField": "%mdc{abc}"
                                  },
                                  "myField2": "%mdc{abc}"
                                }
                            </pattern>
                            <omitEmptyFields>true</omitEmptyFields>
                        </pattern>
                        <logstashMarkers/>
                    </providers>
                </encoder>
            </appender>
            <root level="INFO">
                <appender-ref ref="jsonConsoleAppender"/>
            </root>
        </springProfile>
    </configuration>





    <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder" charset="UTF-8">
              <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                            "app": "${APP_NAME}",
                            "level": "%level",
                            "pid": "${PID:-}",
                            "thread": "%thread",
                            "class": "%logger{40}",
                            "message": "%message",
                            "stack_trace": "%exception{10}"
                        }
                    </pattern>
                </pattern>
              </providers>
            </encoder>

============================================================================================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <contextName>${HOSTNAME}</contextName>
  <property name="LOG_PATH" value="phantom-log" />
  <springProperty scope="context" name="appName" source="spring.application.name" />
  <springProperty scope="context" name="ip" source="spring.cloud.client.ipAddress" />
  <property name="CONSOLE_LOG_PATTERN"
            value="[%d{yyyy-MM-dd HH:mm:ss.SSS} ${ip} ${appName} %highlight(%-5level) %yellow(%X{X-B3-TraceId}),%green(%X{X-B3-SpanId}),%blue(%X{X-B3-ParentSpanId}) %yellow(%thread) %green(%logger) %msg%n"/>

  <appender name="FILEERROR" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>../${LOG_PATH}/${appName}/${appName}-error.log</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>../${LOG_PATH}/${appName}/${appName}-error-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
      <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
        <maxFileSize>2MB</maxFileSize>
      </timeBasedFileNamingAndTriggeringPolicy>
    </rollingPolicy>
    <append>true</append>
    <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
      <pattern>${CONSOLE_LOG_PATTERN}</pattern>
      <charset>utf-8</charset>
    </encoder>
    <filter class="ch.qos.logback.classic.filter.LevelFilter">
      <level>error</level>
      <onMatch>ACCEPT</onMatch>
      <onMismatch>DENY</onMismatch>
    </filter>
  </appender>

  <appender name="FILEWARN" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>../${LOG_PATH}/${appName}/${appName}-warn.log</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>../${LOG_PATH}/${appName}/${appName}-warn-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
      <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
        <maxFileSize>2MB</maxFileSize>
      </timeBasedFileNamingAndTriggeringPolicy>
    </rollingPolicy>
    <append>true</append>
    <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
      <pattern>${CONSOLE_LOG_PATTERN}</pattern>
      <charset>utf-8</charset>
    </encoder>
    <filter class="ch.qos.logback.classic.filter.LevelFilter">
      <level>warn</level>
      <onMatch>ACCEPT</onMatch>
      <onMismatch>DENY</onMismatch>
    </filter>
  </appender>

  <appender name="FILEINFO" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>../${LOG_PATH}/${appName}/${appName}-info.log</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>../${LOG_PATH}/${appName}/${appName}-info-%d{yyyy-MM-dd}.%i.log</fileNamePattern>
      <timeBasedFileNamingAndTriggeringPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
        <maxFileSize>2MB</maxFileSize>
      </timeBasedFileNamingAndTriggeringPolicy>
    </rollingPolicy>
    <append>true</append>
    <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
      <pattern>${CONSOLE_LOG_PATTERN}</pattern>
      <charset>utf-8</charset>
    </encoder>

    <filter class="ch.qos.logback.classic.filter.LevelFilter">
      <level>info</level>
      <onMatch>ACCEPT</onMatch>
      <onMismatch>DENY</onMismatch>
    </filter>
  </appender>

  <appender name="logstash" class="ch.qos.logback.core.rolling.RollingFileAppender">
    <file>../${LOG_PATH}/${appName}/${appName}.json</file>
    <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
      <fileNamePattern>../${LOG_PATH}/${appName}/${appName}-%d{yyyy-MM-dd}.json</fileNamePattern>
      <maxHistory>7</maxHistory>
    </rollingPolicy>
    <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
      <providers>
        <timestamp>
          <timeZone>UTC</timeZone>
        </timestamp>
        <pattern>
          <pattern>
            {
            "ip": "${ip}",
            "app": "${appName}",
            "level": "%level",
            "trace": "%X{X-B3-TraceId:-}",
            "span": "%X{X-B3-SpanId:-}",
            "parent": "%X{X-B3-ParentSpanId:-}",
            "thread": "%thread",
            "class": "%logger{40}",
            "message": "%message",
            "stack_trace": "%exception{10}"
            }
          </pattern>
        </pattern>
      </providers>
    </encoder>
  </appender>

  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>${CONSOLE_LOG_PATTERN}</pattern>
      <charset>utf-8</charset>
    </encoder>
    <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
      <level>debug</level>
    </filter>
  </appender>

  <logger name="org.springframework" level="INFO" />
  <logger name="org.hibernate" level="INFO" />
  <logger name="com.kingboy.repository" level="DEBUG" />

  <root level="INFO">
    <appender-ref ref="FILEERROR" />
    <appender-ref ref="FILEWARN" />
    <appender-ref ref="FILEINFO" />
    <appender-ref ref="logstash" />
    <appender-ref ref="STDOUT" />
  </root>
</configuration>

=========================================================================================================================
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <property resource="application.yml" />
    <contextName>${spring.application.name}</contextName>
    <appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <fieldName>timeStamp</fieldName>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <loggerName>
                    <fieldName>logger</fieldName>
                </loggerName>
                <logLevel>
                    <fieldName>level</fieldName>
                </logLevel>
                <threadName>
                    <fieldName>thread</fieldName>
                </threadName>
                <mdc />
                <message>
                    <fieldName>message</fieldName>
                </message>
                <arguments>
                    <includeNonStructuredArguments>false</includeNonStructuredArguments>
                </arguments>
                <stackTrace>
                    <fieldName>stack</fieldName>
                </stackTrace>
            </providers>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="CONSOLE" />
    </root>
</configuration>











<configuration>
    <appender name="out-json" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <fieldName>ts</fieldName>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <loggerName>
                    <fieldName>logger</fieldName>
                </loggerName>
                <logLevel>
                    <fieldName>severity</fieldName>
                </logLevel>
                <callerData>
                    <classFieldName>class</classFieldName>
                    <methodFieldName>method</methodFieldName>
                    <lineFieldName>line</lineFieldName>
                    <fileFieldName>file</fileFieldName>
                </callerData>
                <threadName>
                    <fieldName>thread</fieldName>
                </threadName>
                <mdc/>
                <arguments>
                    <includeNonStructuredArguments>false</includeNonStructuredArguments>
                </arguments>
                <stackTrace>
                    <fieldName>stack</fieldName>
                </stackTrace>
                <message>
                    <fieldName>message</fieldName>
                </message>
            </providers>
        </encoder>
    </appender>

    <appender name="out-console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <root level="info">
        <!--
        Can be custom your own environment (LOG_APPENDER)
        to load a specify appender ( out-json or out-console ).
        Default value: out-json
        -->
        <appender-ref ref="${LOG_APPENDER:-out-json}"/>
    </root>
</configuration>












<configuration debug="true" scan="true" scanPeriod="10 minutes">
<!--    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%green(example-project) - %cyan(%date [%thread] %-5level %logger{36}) - %message : %magenta([DEVICE_ID=%X{device_id}] [DEVICE_NAME=%X{device_name}] [DEVICE_OS=%X{device_os}] [APP_VER=%X{app_version}] [DEVICE_LANG=%X{device_language}] [IP=%X{ip}] [PORT=%X{port}] [USER_AGENT=%X{user_agent}] [AUTH=%X{auth}] [TX_ID=%X{tx_id}] [CLIENT_ID=%X{client_id}]%n)
            </pattern>
        </encoder>
    </appender>     -->

   <appender name="DAILY_APP_LOG_FILE" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <file>/Users/tcnseri/Documents/applogs.log</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <fileNamePattern>/Users/senoritadev/Documents/applogs.%d{yyyy-MM-dd}.log</fileNamePattern>
        </rollingPolicy>
        <encoder>
            <pattern>%green(example-project) - %cyan(%date [%thread] %-5level %logger{36}) - %message : %magenta([DEVICE_ID=%X{device_id}] [DEVICE_NAME=%X{device_name}] [DEVICE_OS=%X{device_os}] [APP_VER=%X{app_version}] [DEVICE_LANG=%X{device_language}] [IP=%X{ip}] [PORT=%X{port}] [USER_AGENT=%X{user_agent}] [AUTH=%X{auth}] [TX_ID=%X{tx_id}] [CLIENT_ID=%X{client_id}]%n)
            </pattern>
        </encoder>
    </appender>


    <appender name="STASH" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:4560</destination>
        <!-- You can add multiple destination values -->
<!--        <destination>100.100.10.100:55525</destination>
        <destination>100.100.10.101:55525</destination>       -->
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp/>
                <version/> <!-- Logstash json format version, the @version field in the output-->
                <mdc/> <!-- MDC variables on the Thread will be written as JSON fields-->
                <context/> <!--Outputs entries from logback's context -->
                <logLevel/>
                <loggerName/>
                <pattern> <!-- we can add some custom fields to be sent with all the log entries. make filtering easier in Logstash-->
                    <pattern>
                        {
                            "appName": "example-project"
                        }
                    </pattern>
                </pattern>
                <threadName/>
                <message/>
                <logstashMarkers/> <!-- Useful so we can add extra information for specific log lines as Markers-->
                <arguments/> <!--or through StructuredArguments-->
                <stackTrace/>
            </providers>
        </encoder>
    </appender>

    <appender name="Sentry" class="io.sentry.logback.SentryAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>WARN</level>
        </filter>
    </appender>

    <logger name="com.yaanimail.eventsocket" level="DEBUG" additivity="false">
        <appender-ref ref="DAILY_APP_LOG_FILE" />
        <appender-ref ref="STASH" />
    </logger>

    <logger name="org.springframework" level="INFO"  additivity="false" >
        <appender-ref ref="DAILY_APP_LOG_FILE" />
    </logger>

    <logger name="org.springframework.data" level="DEBUG"  additivity="false" >
        <appender-ref ref="DAILY_APP_LOG_FILE" />
    </logger>

    <logger name="org.springframework.transaction" level="DEBUG"  additivity="false" >
        <appender-ref ref="DAILY_APP_LOG_FILE" />
    </logger>

    <logger name="org.hibernate" level="INFO"  additivity="false" >
        <appender-ref ref="DAILY_APP_LOG_FILE" />
    </logger>

    <root level="TRACE">
        <!--<appender-ref ref="STDOUT"/>-->
        <appender-ref ref="Sentry"/>
        <appender-ref ref="DAILY_APP_LOG_FILE"/>
    </root>
</configuration>





<providers>
        <mdc/>
        <context/>
        <version/>
        <logLevel/>
        <loggerName/>
        <threadName/>
        <provider class="com.foreach.across.modules.logging.request.RequestLoggerJsonProvider"/>
        <logstashMarkers/>
        <arguments/>
        <stackTrace/>
        <pattern>
            <pattern>
                {
                "application": "${logging.logstash.application}",
                "appender": "requests"
                }
            </pattern>
        </pattern>
</providers>



<providers>
        <mdc/>
        <context/>
        <version/>
        <logLevel/>
        <loggerName/>
        <threadName/>
        <message/>
        <logstashMarkers/>
        <arguments/>
        <stackTrace/>
        <pattern>
            <pattern>
                {
                "application": "${logging.logstash.application}",
                "appender": "errors"
                }
            </pattern>
        </pattern>
</providers>
























How Is Docker Logging Different: = https://skillfield.com.au/blog/monitoring-kubernetes-and-docker-container-logs/
Most conventional log analysis methods don’t work on containerized logging – t
roubleshooting becomes more complex compared to traditional hardware-centric apps that run on a single node and need less troubleshooting. You need more data to work with so you must extend your search to get to the root of the problem.

Here’s why:
Containers are Ephemeral
Docker containers emit logs to the stdout and stderr output streams. Because containers are stateless, the logs are stored on the Docker host in JSON files by default. Why?
The default logging driver is JSON-file.
The logs are then annotated with the log origin, either stdout or stderr, and a timestamp. Each log file contains information about only one container.

You can find these JSON log files in the/var/lib/docker/containers/directory on a Linux Docker host. Here’s how you can access them:

That’s where logging comes into play. You can collect the logs with a log aggregator and store them in a place where they’ll be available forever.
It’s dangerous to keep logs on the Docker host because they can build up over time and eat into your disk space.
That’s why you should use a central location for your logs and enable log rotation for your Docker containers.


Get Started with Docker Container Logs
When you’re using Docker, you work with two different types of logs: daemon logs and container logs.

What Are Docker Container Logs?
Docker container logs are generated by the Docker containers. They need to be collected directly from the containers.
Any messages that a container sends to stdout or stderr is logged then passed on to a logging driver that forwards them to a remote destination of your choosing.

Here are a few basic Docker commands to help you get started with Docker logs and metrics:
-------------------------------------------------------------------------------------------https://skillfield.com.au/blog/monitoring-kubernetes-and-docker-container-logs/
Show container logs: docker logs containerName
Show only new logs: docker logs -f containerName
Show CPU and memory usage: docker stats
Show CPU and memory usage for specific containers: docker stats containerName1 containerName2
Show running processes in a container: docker top containerName
Show Docker events: docker events
Show storage usage: docker system df

- Watching logs in the console is nice for development and debugging,
- however in production you want to store the logs in a central location for search, analysis, troubleshooting and alerting.



public class Example {
    final Logger log = LoggerFactory.getLogger(Example.class);
}
OR=========
@Slf4j
public class Example {
}
- log.debug("Found {} results", list.size());// log.debug("Found [{}] results", list.size());
- Logback, by default, will produce logs in plain text.

How to use Sleuth?
-----------------
Sleuth is part of spring cloud libraries. It can be used to generate the traceid, spanid and add this information to the service calls in headers
and mapped diagnostic context (MDC).
These ids can be used by the tools such as Zipkin, ELK to store, index and process the log file.
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-sleuth</artifactId>
    </dependency>
- The default pattern set by Spring Cloud Sleuth is: logging.pattern.level=%5p [${spring.zipkin.service.name:${spring.application.name:-}},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-Span-Export:-}]
- logging.pattern.level=%5p [${spring.zipkin.service.name:${spring.application.name:-}},%X{X-B3-TraceId:-}]
- logging.pattern.level: set to %5p [${spring.zipkin.service.name:${spring.application.name:-}},%X{X-B3-TraceId:-},%X{X-B3-SpanId:-},%X{X-Span-Export:-}]

How to use Zipkin?
Zipkin contains two components:
-------------------------------
Zipkin Client
Zipkin Server

To use zipkin client following dependency needs to be added in the application
    <dependency>
        <groupId>org.springframework.cloud</groupId>
        <artifactId>spring-cloud-starter-zipkin</artifactId>
    </dependency>

Download and run zipkin server : java -jar zipkin-server-2.12.9-exec.jar

spring.zipkin.base-url=http://localhost:9411
spring.zipkin.enabled=true
spring.zipkin.service.name=xyz-service
spring.sleuth.enabled=true
spring.sleuth.integration.enabled=true
spring.sleuth.sampler.rate=100
spring.sleuth.sampler.probability=1.0
spring.zipkin.sender.type=web

Export logs in JSON:
- we can use the Logstash Logback Encoder or LoggingEventCompositeJsonEncoder.
- In general, the microservices will run in Docker containers so that we can leave the responsibility of writing the log files to Docker.

- Instead of managing log files directly, our microservices could log to the standard output using the ConsoleAppender.
- As the microservices will run in Docker containers, we can leave the responsibility of writing the log files to Docker.
- Docker writes the container logs in files. FileBeat then reads those files and transfer the logs into ElasticSearch.
  FileBeat is used as a replacement for Logstash. It was created because Logstash requires a JVM and tends to consume a lot of resources.
  Although FileBeat is simpler than Logstash, you can still do a lot of things with it.
- ElasticSearch has a volume to keep its data. Kibana does not need a volume as it uses ElasticSearch to persist its configuration.
- Also containers are immutable and ephemeral, which means they have a shorter life span.
- In Linux by default docker logs can be found in this location:  /var/lib/docker/containers/<container-id>/<container-id>-json.log
- Filebeat will then extract logs from that location and push them towards Logstash


Filebeat
--------------
FileBeat on the other hand needs a specific configuration file to achieve what we want.
- By default standard output(stdout) off all docker containers is written into json files.
- These log files are stored on the host where the docker engine is running
- can be found under the following path /var/lib/docker/containers/{container-id}/{container-id}-json.log
- To share this configuration file with the container, we need a read-only volume /usr/share/filebeat/filebeat.yml:ro.
- FileBeat also needs to have access to the docker log files.
- You can usually find them in /var/lib/docker/containers but that may depends on your docker installation.
- The docker socket /var/run/docker.sock is also shared with the container.
- FileBeat to use the docker daemon to retrieve information and enrich the logs with things that are not directly in the log files, such as the name of the image or the name of the container.
- The user running FileBeat needs to be able to access all these shared elements. Unfortunately, the user filebeat used in the official docker image does not have the privileges to access them.
  That is why the user was changed to root in the docker compose file.
- Filebeat has to be run in background in the compute Nodes that are running K8s.
- To make sure that a Filebeat Pod is scheduled on each and every Node, another K8s construct called DaemonSet has to be used.
- A DaemonSet makes sure that the Filebeat Pods are scheduled in every Node with a count of 1.

STEP 1 - INSTALL FILEBEAT
STEP 2 - LOCATE THE CONFIGURATION FILE
$ /etc/filebeat/filebeat.yml
Change the owner of the filebeat.yml file to root to allow access to the docker container logs.
$ sudo chown root:root filebeat.yml
$ ls -la
STEP 3 - CONFIGURE THE INPUTS
On Linux we want filebeat to read the container logs from /var/lib/docker/containers/*/*.log which is where docker's container logs are stored, this is handled by default.
Add to your filebeat.inputs section the docker type.
------------------------
filebeat.inputs:
- type: docker
  containers.ids:
    - '*'
STEP 4 - ENABLE THE INPUT
STEP 5 - CONFIGURE OUTPUT
STEP 6 - VALIDATE CONFIGURATION
$ sudo filebeat -e -c /etc/filebeat/filebeat.yml
STEP 7 - START FILEBEAT



# The Logging driver has been set to JSON-file. In the future, if the labels is set to true the logs generated would store as a json file format.
logging:
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "2"


Q: Why are logs stored in JSON files?
A: The default logging driver is json-file.

Q: What’s a logging driver?
A: logging driver is a mechanism for getting info from your running containers.(default json-file, like syslog, journald, fluentd, or logagent.)

  It’s dangerous to keep logs on the Docker host because they can build up over time and eat into your disk space. 
  That’s why you should use a central location for your Docker logs and enable log rotation for your containers.


= Log parser to structure logs, typically part of log shippers (fluentd, rsyslog, logstash, logagent, …)
= Log indexing, visualisation and alerting:
    Elasticsearch and Kibana (Elastic Stack, also known as ELK stack),
    Splunk,
    Logentries,
    Loggly,
    Sumologic,
    Graylog  OSS / Enterprise
    Sematext Cloud / Enterprise
    and many more  …  


So we must change the ownership of our config file
$ sudo chown root conf/filebeat.docker.yml

and revoke all write permissions, but that of the owner:
$ sudo chmod go-w conf/filebeat.docker.yml

Install Filebeat
To add filebeat to our compose file, we'll have to add the following volume mounts:
------------------------------------------------------------------------
binding our configuration to /usr/share/filebeat/filebeat.yml
binding the location of docker container directory /var/lib/docker/containers to the container
binding our docker socket /var/run/docker.sock to the container
In order for the container to access these resources, it must use the root user.

Add the following service to your compose file:

version: "2.4"
services:
  filebeat/log-shipper:
    image: "docker.elastic.co/beats/filebeat:7.2.0"
    user: root
    volumes:
      - /MY_WORKDIR/filebeat.docker.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker:/var/lib/docker:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./filebeat/data:/usr/share/filebeat/data:rw
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    restart: unless-stopped
    container_name: filebeat
    labels:
      co.elastic.logs/enabled: "false"   #for the filebeat container to prevent the logs generated from this container from being ingested

/MY_WORKDIR/filebeat.docker.yml:
======================================https://coralogix.com/blog/filebeat-configuration-best-practices-tutorial/
filebeat.inputs:
- type: container
  paths:
    - '/var/lib/docker/containers/*/*.log'
processors:
- add_docker_metadata:
    host: "unix:///var/run/docker.sock"
- decode_json_fields:
    fields: ["message"]
    target: "json"
    overwrite_keys: true
output.console:
   pretty: true
output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  protocol: "https"
  username: "elastic_username"
  password: "elastic_password"
  indices:
    - index: "filebeat-%{[agent.version]}-%{+yyyy.MM.dd}"
logging.json: true
logging.metrics.enabled: false

With this, the Filebeat image and the configuration has been set up. Let’s go ahead and create a Dockerfile in the same directory.
Dockerfile
----------------------
FROM docker.elastic.co/beats/filebeat:9.0.1
#Copy our custom configuration file
COPY filebeat.docker.yml /usr/share/filebeat/filebeat.yml:ro
USER root
#Create a directory to map volume with all docker log files
RUN mkdir /usr/share/filebeat/dockerlogs
RUN chown root:filebeat /usr/share/filebeat/filebeat.yml                      #RUN chown -R root /usr/share/filebeat/
RUN chmod go-w /usr/share/filebeat/filebeat.yml                               #RUN chmod -R go-w /usr/share/filebeat/

EX:
Span span = tracer.currentSpan();
logger.info("MDC context map: {}", MDC.getCopyOfContextMap());
if (span != null) {
    logger.info("Trace ID hex {}", span.context().traceIdString());
    logger.info("Trace ID decimal {}", span.context().traceId());
    logger.info("Span ID hex {}", span.context().spanIdString());
    logger.info("Span ID decimal {}", span.context().spanId());
}
EX:
@GetMapping("/hello")
public String hello(@RequestParam String name, @RequestHeader Map<String, String> headers) {
    log.info("headers:{}", headers);
}// headers:{x-b3-traceid=6dd57dc2ad55c58f, x-b3-spanid=b5e6de658b261ac7, x-b3-parentspanid=6dd57dc2ad55c58f, x-b3-sampled=1, accept=*/*, user-agent=Java/1.8.0_202, host=localhost:8081, connection=keep-alive}



###you could use a different log shipper, such as Fluentd or Filebeat, to send the Docker logs to Elasticsearch.
###Or, you could add an additional layer comprised of a Kafka or Redis container to act as a buffer between Logstash and Elasticsearch.

Note = If you use a custom logback-spring.xml, you must pass the spring.application.name in the bootstrap rather than the application property file.
       Otherwise, your custom logback file does not properly read the property.
resources/logback-spring.xml  https://cassiomolin.com/2019/06/30/log-aggregation-with-spring-boot-elastic-stack-and-docker/
==============================https://cassiomolin.com/2019/06/30/log-aggregation-with-spring-boot-elastic-stack-and-docker/#logging-in-json-format
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <springProperty scope="context" name="application_name" source="spring.application.name"/>

    <appender name="jsonConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"/>
    </appender>

    <root level="INFO">
        <appender-ref ref="jsonConsoleAppender"/>
    </root>
</configuration>

output:                    https://cassiomolin.com/2019/06/30/log-aggregation-with-spring-boot-elastic-stack-and-docker/
---------------------------https://cassiomolin.com/2019/06/30/log-aggregation-with-spring-boot-elastic-stack-and-docker/#logging-in-json-format
{
   "@timestamp": "2019-06-29T23:01:38.967+01:00",
   "@version": "1",
   "message": "Finding details of post with id 1",
   "logger_name": "com.cassiomolin.logaggregation.post.service.PostService",
   "thread_name": "http-nio-8001-exec-3",
   "level": "INFO",
   "level_value": 20000,
   "application_name": "post-service",
   "traceId": "c52d9ff782fa8f6e",
   "spanId": "c52d9ff782fa8f6e",
   "spanExportable": "false",
   "X-Span-Export": "false",
   "X-B3-SpanId": "c52d9ff782fa8f6e",
   "X-B3-TraceId": "c52d9ff782fa8f6e"
}
- When Spring Cloud Sleuth is in the classpath,
  the following properties will added to MDC and will be logged: traceId, spanId, spanExportable, X-Span-Export, X-B3-SpanId and X-B3-TraceId.
- If we need more flexibility in the JSON format and in data included in log, we can use LoggingEventCompositeJsonEncoder.
- The composite encoder has no providers configured by default, so we must add the providers we want to customize the output
- The Logstash Logback Encoder was originally written to support output in Logstash’s JSON format,


https://stacktobasics.com/correlation-ids
.............................................
By default, Spring Cloud Sleuth will add the trace ID as a header when calling other services, 
which is how the distributed trace is tracked. 
However, it does not automatically add the trace id to responses from our application. 
This can easily be added with a filter, which we'll show below.


resources/logback-spring.xml
=============================https://cassiomolin.com/2019/06/30/log-aggregation-with-spring-boot-elastic-stack-and-docker/=
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <springProperty scope="context" name="application_name" source="spring.application.name"/>

    <appender name="jsonConsoleAppender" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <version/>
                <logLevel/>
                <message/>
                <loggerName/>
                <threadName/>
                <context/>
                <pattern>
                    <omitEmptyFields>true</omitEmptyFields>
                    <pattern>
                        {
                            "trace": {
                                "trace_id": "%mdc{X-B3-TraceId}",
                                "span_id": "%mdc{X-B3-SpanId}",
                                "parent_span_id": "%mdc{X-B3-ParentSpanId}",
                                "exportable": "%mdc{X-Span-Export}"
                            }
                        }
                    </pattern>
                </pattern>
                <mdc>
                    <excludeMdcKeyName>traceId</excludeMdcKeyName>
                    <excludeMdcKeyName>spanId</excludeMdcKeyName>
                    <excludeMdcKeyName>parentId</excludeMdcKeyName>
                    <excludeMdcKeyName>spanExportable</excludeMdcKeyName>
                    <excludeMdcKeyName>X-B3-TraceId</excludeMdcKeyName>
                    <excludeMdcKeyName>X-B3-SpanId</excludeMdcKeyName>
                    <excludeMdcKeyName>X-B3-ParentSpanId</excludeMdcKeyName>
                    <excludeMdcKeyName>X-Span-Export</excludeMdcKeyName>
                </mdc>
                <stackTrace/>
            </providers>
        </encoder>
    </appender>

    <root level="INFO">
        <appender-ref ref="jsonConsoleAppender"/>
    </root>
</configuration>

output
------------------------
{
   "@timestamp": "2019-06-29T22:01:38.967Z",
   "@version": "1",
   "level": "INFO",
   "message": "Finding details of post with id 1",
   "logger_name": "com.cassiomolin.logaggregation.post.service.PostService",
   "thread_name": "http-nio-8001-exec-3",
   "application_name": "post-service",
   "trace": {
      "trace_id": "c52d9ff782fa8f6e",
      "span_id": "c52d9ff782fa8f6e",
      "exportable": "false"
   }
}

Field	        Description
@version	    standard Elasticsearch field set by Logstash
@timestamp	    standard Elasticsearch field set by Logback
host	        set by Logstash
port	        set by Logstash
HOSTNAME	    set by Logback
@app	        custom field set by configuration (the name of the origin microservice)
@type	        custom field set by configuration (type of the log)
logger_name	    set by Logback
level	        set by Logback
level_value	    set by Logback
message	        set by Logback
thread_name 	set by Logback
stack_trace	    set by Logback, content valuated by the ShortenedThrowableConverter component
stack_hash	    custom field set by the StackHashJsonProvider component
userId	        custom MDC field set by the PrincipalFilter component
sessionId	    custom MDC field set by the SessionIdFilter component
X-B3-TraceId	MDC field set by Spring Cloud Sleuth for logs correlation
X-B3-SpanId	    MDC field set by Spring Cloud Sleuth for logs correlation
X-Span-Export	MDC field set by Spring Cloud Sleuth for logs correlation


filter {
   # pattern matching logback pattern
   grok {
          match => { "message" => "(?m)OUT\s+%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:severity}\s+\[%{DATA:service},%{DATA:trace},%{DATA:span},%{DATA:exportable}\]\s+%{DATA:pid}\s+---\s+\[%{DATA:thread}\]\s+%{DATA:class}\s+:\s+%{GREEDYDATA:rest}" }
   }
}

filter {
   # pattern matching logback pattern
   grok {
          match => { "message" => "%{TIMESTAMP_ISO8601:timestamp}\s+%{LOGLEVEL:severity}\s+\[%{DATA:service},%{DATA:trace},%{DATA:span},%{DATA:exportable}\]\s+%{DATA:pid}\s+---\s+\[%{DATA:thread}\]\s+%{DATA:class}\s+:\s+%{GREEDYDATA:rest}" }
   }
}

















Deploying Filebeat DaemonSet
https://systemadminspro.com/logs-collection-and-parsing-using-filebeat/

Elasticsearch Dockerfile
FROM docker.elastic.co/elasticsearch/elasticsearch:6.5.2
COPY --chown=elasticsearch:elasticsearch elasticsearch.yml /usr/share/elasticsearch/config/
CMD ["elasticsearch", "-Elogger.level=INFO"]

Logstash Dockerfile
FROM docker.elastic.co/logstash/logstash:6.5.2
RUN rm -f /usr/share/logstash/pipeline/logstash.conf
COPY pipeline/ /usr/share/logstash/pipeline/

Logstash conf
input {
    beats {
        port => 5044
        host => "0.0.0.0"
      }
    }

output {
    elasticsearch {
        hosts => elasticsearch
        manage_template => false
            index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
    }
   stdout { codec => rubydebug }
}

9322221919

name:"9322221919,FT3"
name:["9322221919","FT3"]

= In Active Logging, the microservice participates in the logging infrastructure.
  - making network connections to intermediate aggregators, sending data to third-party logging services (e.g. Loggly), 
    or writing directly to a database or index. If the microservice does anything other than output logs to stdout or stderr
= Microservices that use Passive Logging, on the other hand, are unaware of the logging infrastructure and simply log messages to standard outputs


import com.ybl.ypp.shared.enumeration.VoucherChannelEnum;
import com.ybl.ypp.shared.enumeration.VoucherStatus;
import com.ybl.ypp.shared.enumeration.ServiceProviderEnum;

remarks="XXXXXXXXXXXXXXXXXXX"

private final String[] csvHeaders = new String[] {"ID","Category Identifier","Category Name","Status","Channel","Priority","Created Date","Last Modified Date"};
====BrandVoucherCategories====                                                              
categoryIdentifier(String)                                                   
categoryName(String)                                                     
categoryIcon(String)                                                   
iconBackground(String)                                                 
status(VoucherStatus) ACTIVE,INACTIVE                                        
channel(VoucherChannelEnum) YPLITE,YPNEXT,YPHUB,YPLITE_WEB,YPLITE_APP             
priority(Integer)                                                     



private final String[] csvHeaders = new String[] {"ID","Service Provider","Brand Product Code","Category Name","Brand Name","Brand Type",
"Denomination List","Terms And Conditions","Imp Instruction","Redemption Steps","Is Stock Available","Service Provider Discount",
"Validity","Voucher Type","Created Date","Last Modified Date"};
====BrandVoucherCatalogue====
serviceProvider(ServiceProviderEnum) VOUCHAGRAM
brandProductCode(String)
categoryName(String)
brandName(String)
termsAndConditions(String)
impInstruction(String)
isStockAvailable(Boolean)
iconLink(String)
serviceProviderDiscount(BigDecimal)
validity(Instant)
voucherType(VoucherTypeEnum) – ONLINE,OFFLINE,BOTH 
brandType(String)
denominationList(String)
redemptionSteps(String)


private final String[] csvHeaders = new String[] {"ID","Channel","Brand Voucher Category Id","Service Provider","Brand Voucher Id","YPP Brand Alias",
"Priority","YPP Discount","Status","Created Date","Last Modified Date"};
====VoucherBrandCategoryMapping====
brandVoucherId(M2O) - ID of BrandVoucherCatalogue table
brandVoucherCategoryId(M2O)  – ID of BrandVoucherCategories table
channel(VoucherChannelEnum) YPLITE,YPNEXT,YPHUB,YPLITE_WEB
serviceProvider(ServiceProviderEnum) VOUCHAGRAM
yppBrandAlias(String)
priority(Integer)
yppDiscount(BigDecimal)
channelIcon(String)
status(VoucherStatus) ACTIVE,INACTIVE 



private final String[] csvHeaders = new String[] {"ID","Channel","Partner Reference Number","Voucher Guid","Voucher Number","Voucher Value",
"Expiry","Bill Transaction Id","Voucher Brand Catalogue Id","Brand Category Id","Created Date","Last Modified Date"};
====BrandVoucherDetails====
channel(VoucherChannelEnum) YPLITE,YPNEXT,YPHUB,YPLITE_WEB
partnerReferenceNumber(String)       ..          
voucherGuid(String)                  ..
voucherNumber(Integer)               ..
voucherCode(String)                  ..
voucherPin(String)                   ..
voucherValue(String)                 ..
expiry(Instant)                      ..    

billTransactionId(M2O) –  ID of BillTransaction table
voucherBrandCatalogueId(M2O) – ID of BrandVoucherCatalogue table
brandCategoryId(M2O) – Id of BrandVoucherCategories table





   
























