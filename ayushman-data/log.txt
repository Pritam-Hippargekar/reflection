<pattern>
    <pattern>
        {
        "timestamp": "%d{yyyy-MM-dd'T'HH:mm:ss.SSSZ}",  
        "severity": "%level",
        "service": "${springAppName:-}",
        "trace": "%X{X-B3-TraceId:-}",
        "span": "%X{X-B3-SpanId:-}",
        "exportable": "%X{X-Span-Export:-}",
        "pid": "${PID:-}",
        "thread": "%thread",
        "class": "%logger{40}",
        "rest": "%message",
        "message": "%msg",
        "level":"%p",
        "logger": "%logger",
        "host": "%property",
        "appname":"springcloud_consume",
        "serverName":"${server-name}"
        "stackTrace":"%ex",
        "ip": "%X{ip}",
        "url": "%X{url}",
        "method": "%X{method}",
        "arg": "%X{arg}",
        "throwable": "%X{throwable}",
        }
    </pattern>
</pattern>

https://www.jianshu.com/p/576723ce2e3a

Each microservice logs through console and we’ll use the Docker fluentd logging driver to process these logs and send them to our elasticsearch.

Trace Id: A unique ID that remains the same throughout the request containing multiple microservices.
Span Id: A unique ID per microservice.
Spring Cloud Sleuth:
----------------------
It lets you track the progress of subsequent microservices by adding the appropriate headers to the HTTP requests.
The library is based on the MDC (Mapped Diagnostic Context) concept, where you can easily extract values put to context and display them in the logs.
If we inspect the http-headers in the request to XXXXController we'll see the following:
------------------------------------------------------------
"x-b3-traceid"="e40993b7256eda9d"
"x-b3-spanid"="0bcc87a9c8ea5fe3"
"x-b3-parentspanid"="71b4a062bf8f970c"


Distributed tracing platforms like OpenZipkin record trace data. 
Trace data is composed of a parent:child tree structure called a Directed Acyclic Graph (DAG for short). 
A root node represents the trace or overall journey, and each span represents an individual hop along the service route. 

Once you have Spring Cloud Sleuth on the classpath, it will automatically instrument common communication channels such as:
----------------------------------------------
Messaging like Kafka or RabbitMQ
HTTP headers via WebMVC and WebFlux controllers
Request headers made through WebClient, and RestTemplate


STEP 7: Configuring microservices to send logs to logstash
------------------------------------------------------------
Sending microservice logs to logstash requires the following dependencies to be added to each and every microservice.
<dependency>
    <groupId>net.logstash.logback</groupId>
    <artifactId>logstash-logback-encoder</artifactId>
    <version>5.3</version>
</dependency>
<dependency>
    <groupId>ch.qos.logback</groupId>
    <artifactId>logback-core</artifactId>
    <version>1.2.3</version>
</dependency>
The next configuration to add is to create a file called logback.xml in resource folder of every microservice with the following contents:
<?xml version="1.0" encoding="UTF-8"?>
<configuration debug="false">
    <include resource="org/springframework/boot/logging/logback/base.xml"/>
    <appender name="logstash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5044</destination>
        <encoder class="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <mdc/>
                <context/>
                <version/>
                <logLevel/>
                <loggerName/>
                <message/>
                <pattern>
                    <pattern>
                        {
                            "serviceName": "account-service"
                        }
                    </pattern>
                </pattern>
                <threadName/>
                <stackTrace/>
            </providers>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="CONSOLE"/>
        <appender-ref ref="logstash"/>
    </root>
    <logger name="org.springframework" level="INFO"/>
    <logger name="com.cinema" level="INFO"/>
</configuration>
The steps outlined above if followed diligently will enable you to put in a place distributed tracing in your micro services architectures and be able to visualise your logs through kibana and search through them using elasticsearch.



logback-spring.xml
=================
<? xml version = "1.0" encoding = "UTF-8" ?> 
<configuration> 
    <include resource = "org/springframework/boot/logging/logback/defaults.xml" />

    <springProperty scope = "context" name = "springAppName" source = "spring.application.name" /> 
    <property name = "CONSOLE_LOG_PATTERN" value = "%clr(%d{yyyy-MM-dd HH:mm:ss. SSS}){faint} %clr(${level:-%5p}) %clr([${springAppName:-},%X{X-B3-TraceId:-},%X{X-B3-SpanId: -},%X{X-Span-Export:-}]){yellow} %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t] ){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}" / >  
    
    <appender name = "CONSOLE" class = "ch.qos.logback.core.ConsoleAppender" > 
       <encoder> <pattern> ${CONSOLE_LOG_PATTERN} </pattern> <charset> UTF-8 </charset> </encoder> 
    < /appender> 
        
    <root level = "INFO" > 
        <appender-ref ref = "CONSOLE" /> 
    </root> 
</configuration>








1. The first springboot-logstash environment construction
1.1 Add maven
        <dependency>
            <groupId>net.logstash.logback</groupId>
            <artifactId>logstash-logback-encoder</artifactId>
            <version>6.3</version>
        </dependency>
1.2 logback-spring.xml adds log printing
    <!--LOGSTASH config -->
    <appender name="LOGSTASH"
        class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5000</destination>
        <!--<encoder charset="UTF-8"
            class="net.logstash.logback.encoder.LogstashEncoder"> -->
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}
                [service:${springAppName:-}]
                [traceId:%X{X-B3-TraceId:-},spanId:%X{X-B3-SpanId:-},parentSpanId:%X{X-B3-ParentSpanId:-},exportable:%X{X- Span-Export:-}]
                [%thread] %-5level %logger{50} - %msg%n</pattern>
            <charset>UTF-8</charset> <!-- Set the character set here -->
        </encoder>
    </appender>
1.3 configure logstash
input {
  tcp {
    port => 5000
  }
}
filter {
  grok {
    match => {
    "message" => "%{TIMESTAMP_ISO8601:logTime} %{GREEDYDATA:service} %{GREEDYDATA:thread} %{LOGLEVEL:level} %{GREEDYDATA:loggerClass}-%{GREEDYDATA:logContent}"}
  }
}
output {
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "springboot-%{+YYYY.MM.dd}"
    user => "elastic"
    password => "changeme"
  }
}








- Spans are timed operations representing small logical units of work. 
  Each span contains some basic information like Start Time, Duration, etc. 
  We can think of Spans as the building blocks of a distribute trace.
    Spans could be
      Request from Loadbalance to Order Service
      Request Order Service to Product Service
      Product Service to Database
      Order Service to Database
- Trace is nothing but a complete execution path of a request and can be thought of as a collection of Directed Acyclic Graph of Spans.
  (Trace is the entire HTTP Request/Response)



What you are seeing there, are the respective parameters:[food-order-publisher,888114b702f9c3aa,888114b702f9c3aa,true]
-------------------------------------------------------------
appname – the name of the application that logged the span
traceId – the id of the latency graph that contains the span
spanId – the id of a specific operation
exportable – whether the log should be exported to Zipkin or not (more about Zipkin later)



Zipkin Client
==================
Zipkin client contains Sampler which collects data from ms apps with the help of sleuth and provides it the zipkin server.
<dependency>
    <groupId>org.springframework.cloud</groupId>
    <artifactId>spring-cloud-starter-zipkin</artifactId>
</dependency>

spring.zipkin.service.name=food-order-consumer
spring.zipkin.sender.type=web             # we want to report data to Zipkin via HTTP calls rather than a message queue  
spring.zipkin.baseUrl=http://localhost:9411
spring.sleuth.sampler.percentage=1.0      # Default is 0.1 which means 10%

@Bean
fun sleuthTraceSampler(): Sampler {
    return Sampler.ALWAYS_SAMPLE
}

Spring Boot App → Log File → Logstash → Elasticsearch. 
There are other ways of accomplishing the same thing, such as configuring logback to use TCP appender to send logs to a remote Logstash instance via TCP, 
and many other configurations.
event will either be a single log line or multiline log event grouped according to the rules described

try {
  parseXML();
} catch (Exception e) {
    // documentId is a StructuredArgument for structured logging, e is used for stacktrace
    logger.error("Parsing xml document failed!", kv("documentId", documentId), e);
    logger.error("Parsing xml document {} failed, underlying cause was [{}]", documentId, e.getMessage(), e);
}





input {
  file {
    path => "/var/app/current/logs/javaApp.log"     ## It must be absolute
    mode => "tail"                           ## only read new entries at the end of the file
    type => "java"                           ## it's just additional piece of metadata in case you will use multiple types of log files in the future.
    codec => multiline {
      pattern => "^%{TIMESTAMP_ISO8601} "    ## Each new log event needs to start with date
      negate => true                         ## if it doesn't start with a date
      what => "previous"                     ## then it should be grouped with a previous line
    }
  }
}
filter {
  #If log line contains tab character followed by 'at' then we will tag that entry as stacktrace
  if [message] =~ "\tat" {
    grok {
      match => ["message", "^(\tat)"]
      add_tag => ["stacktrace"]
    }
  }

  #Grokking Spring Boot's default log format
  grok {
    match => [ "message", 
               "(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- \[(?<thread>[A-Za-z0-9-]+)\] [A-Za-z0-9.]*\.(?<class>[A-Za-z0-9#_]+)\s*:\s+(?<logmessage>.*)",
               "message",
               "(?<timestamp>%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME})  %{LOGLEVEL:level} %{NUMBER:pid} --- .+? :\s+(?<logmessage>.*)"
             ]
  }

  #Parsing out timestamps which are in timestamp field thanks to previous grok section
  date {
    match => [ "timestamp" , "yyyy-MM-dd HH:mm:ss.SSS" ]
  }
}









input {
        file {
        path => "/root/mult.log"
        start_position => "beginning"
        sincedb_path => "/dev/null"
        codec => multiline{
                 pattern => "^ -%{SPACE}%{TIMESTAMP_ISO8601}"
                 negate => true
                 what => "previous"
        }
}
 }
filter {
    grok {
     match => [
       "message", "(?m)^ -%{SPACE}%{TIMESTAMP_ISO8601:time} \[%{WORD:main}\] %{LOGLEVEL:loglevel}%{SPACE}\(%{JAVACLASS:class}\) %{DATA:mydata}\n(\t)?%{GREEDYDATA:stack}",
       "message", "^ -%{SPACE}%{TIMESTAMP_ISO8601:time} \[%{WORD:main}\] %{LOGLEVEL:loglevel}%{SPACE}\(%{JAVACLASS:class}\) %{GREEDYDATA:mydata}" ]
        break_on_match => false
 }
    date {
    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z"]
 }
}

output {
  stdout { codec => rubydebug}
elasticsearch {
    host => "localhost"
  }
}








output {
    elasticsearch {
        hosts => "elasticsearch:9200"
        index => "인덱스명지정"
        user => "아이디입력"
        password => "비밀번호입력"
    }
}









input {
  file {
    path => "/usr/logstash/logstash-7.2.0/ml-latest-small/movies.csv" #Pay attention to change to your own directory
    start_position => "beginning" 
    sincedb_path => "/dev/null"
  }
}
filter {
  csv {
    separator => "," 
    columns => ["id","content","genre" ]
  }
  mutate {
    split => { "genre" => "|" }
    remove_field => ["path", "host","@timestamp","message" ]
  }
  mutate {

    split => ["content", "(" ]
    add_field => { "title" => "%{[content][0]}" }
    add_field => { "year" => "%{[content][2]}" }
  }
  mutate {
    convert => {
       "year" => "integer"
    }
    strip => ["title" ]
    remove_field => ["path", "host","@timestamp","message","content" ]
  }
}
output {
   elasticsearch {
     hosts => http: // 192.168.1.204:9200 #Pay attention to modify to your own ES 
     index => "movies" 
     document_id => "%{id}"
   }
  stdout{}
}


<?xml version="1.0" encoding="UTF-8"?>
<!--The log saves log information with different log levels into different files-->
<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml" />

    <!--springProperty: Find the corresponding configuration item in the properties/yml file -->
    <springProperty scope="context" name="springAppName" source="spring.application.name" />
    <springProperty scope="context" name="logFilePath" source="logging.config.path" />

    <!-- The output location of the log in the project -->
    <property name="LOG_FILE" value="${BUILD_FOLDER:-build}/${springAppName}" />

    <!-- console log output style -->
    <property name="CONSOLE_LOG_PATTERN" 
              value ="%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${ PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}" />

    <!-- console output appender-->
    <appender name="console" class ="ch.qos.logback.core.ConsoleAppender">
        <filter class ="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <!-- Log output encoding -->
        <encoder class ="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>${CONSOLE_LOG_PATTERN}</pattern>
            <charset>utf8</charset>
        </encoder>
    </appender>

    <!-- Appender for logstash output in JSON format -->
    <appender name="logstash"
              class ="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>192.168.1.204:9665</destination>
        <!-- Log output encoding -->
        <encoder class ="net.logstash.logback.encoder.LoggingEventCompositeJsonEncoder">
            <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "severity": "%level" ,
                         "service": "${springAppName:-}" ,
                         "trace": "%X{X-B3-TraceId:-}" ,
                         "span": "%X{X- B3-SpanId:-}" ,
                         "exportable": "%X{X-Span-Export:-}" ,
                         "pid": "${PID:-}" ,
                         "thread": "%thread" ,
                         "class ": "%logger{40}" ,
                         "rest": "%message"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
    </appender>

    <!--File format output appender-->
    <appender name="file" class ="ch.qos.logback.core.rolling.RollingFileAppender">
        <!--Define the path of log output-->
        <!--The scheduler.manager.server.home here is not set in the above configuration, so the value configured when java starts will be used-->
        <!--For example, configure this property through java -Dscheduler.manager.server.home=/path/to XXXX -->
        <file>${logging.path}/spring-boot/elk.log</file>
        <!--Define the log rolling strategy-->
        <rollingPolicy class ="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--The format of the file name when defining the file scrolling -->
            <fileNamePattern>${scheduler.manager.server.home}/logs/${app.name}.%d{yyyy-MM- dd.HH}.log
             </fileNamePattern>
            <!--60-day time period, the maximum log volume is 20GB-->
            <maxHistory>60</maxHistory>
            <!-- This attribute is only supported after version 1.1.6 -->
            <totalSizeCap>20GB</totalSizeCap>
        </rollingPolicy>
        <triggeringPolicy class ="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <!--The maximum size of each log file is 100MB-->
            <maxFileSize>100MB</maxFileSize>
        </triggeringPolicy>
        <!--Define output format-->
        <encoder>
            <pattern>%d [%thread] %-5level %logger{36} [%file : %line] - %msg%n</pattern>
        </encoder>
    </appender>

    <!--logger is used to set the log printing level of a certain package or a specific class and specify appender-->
    <!--The logger can be obtained through LoggerFactory.getLogger("mytest")-->
    <!--Since this logger automatically inherits the root appender, there is already a stdout appender in the root, and I have introduced the stdout appender-->
    <!--If additivity="false" is not set, a log will be output twice in the console, and the division of labor is done through appender- ref, and root is responsible for console and logstash
      This logger is responsible for file output -->
    <!--additivity indicates whether to use the appender configured by rootLogger for output -->
    <logger name="test" level="INFO" additivity="false">
        <appender-ref ref="file"/>
    </logger>

    <!-- The root logger is also a kind of logger, and has only one level attribute -->
    <root level="INFO">
        <appender-ref ref="console" />
        <appender-ref ref="logstash" />
    </root>

</configuration>










https://insight-bgh.tistory.com/519
-----------------------------------------

1. Add logstash dependency (build.gradle )
implementation 'net.logstash.logback:logstash-logback-encoder:6.6'
 

2. Add logback-spring.xml file
Add logback-spring.xml under resources folder.

<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="30 seconds">

    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>%-5level %d{HH:mm:ss.SSS} [%thread] %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <encoder>
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS}[%-5level] : %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="ROLLING" class="ch.qos.logback.core.rolling.RollingFileAppender">
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>INFO</level>
        </filter>
        <file>mylog.txt</file>
        <rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!-- rollover daily -->
            <fileNamePattern>mylog-%d{yyyy-MM-dd}.%i.txt</fileNamePattern>
            <timeBasedFileNamingAndTriggeringPolicy
                    class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
                <!-- or whenever the file size reaches 100MB -->
                <maxFileSize>100MB</maxFileSize>
            </timeBasedFileNamingAndTriggeringPolicy>
        </rollingPolicy>
        <encoder>
            <pattern>[%-5level] %d{HH:mm:ss.SSS} %logger{36} - %msg%n</pattern>
        </encoder>
    </appender>

    <!-- Log Stash 사용시 사용할 TCP 통신 정보 -->
    <appender name="stash" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>서버ip정보:logstash포트</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder"></encoder>
    </appender>

   
    <root level="INFO">
        <appender-ref ref="console" />
        <appender-ref ref="stash" />
    </root>

</configuration>
 

3. Add logback-spring.xml to application.yml
logging:
  config: classpath:logback-spring.xml
 

4. Add spring-cloud-starter-sleuth (build.gradle )
If you want to see logs by traceId and spanId, add spring-cloud-starter-sleuth dependency. It is often used to track logs in the msa environment.
implementation group: 'org.springframework.cloud', name: 'spring-cloud-starter-sleuth', version: '3.0.4'






















# spring.application.name and server.port are set in the main methods,
# so not done here
#logging.level.org.springframework.web=DEBUG
spring.sleuth.traceId128=true
spring.zipkin.base-url=http://127.0.0.1:9411/
spring.sleuth.sampler.probability=1.0
# Adds trace and span IDs to logs (when a trace is in progress)
#logging.pattern.level=[%X{traceId}/%X{spanId}] %-5p [%t] %C{2} - %m%n
# Propagates a field named 'user_name' downstream
spring.sleuth.baggage.remote-fields=user_name
spring.application.name=sale
server.port=8081

Logstash
========
Logstash is not just an input | filter | output data flow, 
but an input | decode | filter | encode | output data flow! codec is used to decode and encode events.


Execute the command to check whether the file configuration is OK
------------------------------------------------------logstash -e "input { stdin { } } output { stdout {} }"
logstash -f first-pipeline.conf --config.test_and_exit

Execute the command to run logstash
-------------------------------------
logstash -f first-pipeline.conf --config.reload.automatic

--config.reload.automatic option allows you to automatically enable configuration reloading without restarting Logstash





input {
    file {
        path => ["/var/log/**/*.log", "/var/log/message"] #绝对路径
        type => "system"
        start_position => "beginning"
        sincedb_path => /dev/null  #每次重启自动从头开始读
    }
}
---
input {
    stdin {
        codec => multiline {
            pattern => "^\["
            negate => true
            what => "previous"
        }
    }
}


https://elasticsearch.tutorials24x7.com/blog/how-to-install-elasticsearch-kibana-and-logstash-elk-elastic-stack-on-windows


https://blog.frankel.ch/structuring-data-with-logstash/
Indexing unstructured log messages is not very useful. Logstash configuration allows to pre-parse unstructured data and send structured data instead.
Grok
Grokking data is the usual way to structure data with pattern matching.

Dissect
The Grok filter gets the job done. But it seems to suffer from performance issues, especially if the pattern doesn’t match. 
An alternative is to use the dissect filter instead, which is based on separators.
it’s much easier to write a separator-based filter than a regex-based one.

filter {
  dissect {
	mapping => { "message" => ... }
  }
  mutate {
    strip => [ "log", "class" ]                        strip additional spaces with Logstash
  }
}





Log file configuration
-----bin/logstash -e 'input{stdin{}}output{stdout{codec=>rubydebug}}'----------------------------------------------------------------------------logstash -e 'input { stdin { } } output { stdout {} }'
<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/base.xml" />
    <!-- 按照每天生成日志文件 -->
    <appender name="FILE"
              class="ch.qos.logback.core.rolling.RollingFileAppender">
        <rollingPolicy
                class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
            <!--日志文件输出的文件名 -->
            <FileNamePattern>${user.dir}/logs/user-%d{yyyyMMdd}.log
            </FileNamePattern>
            <!--日志文件保留天数 -->
            <MaxHistory>30</MaxHistory>
        </rollingPolicy>
        <!--日志文件最大的大小 -->
        <triggeringPolicy
                class="ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy">
            <MaxFileSize>10MB</MaxFileSize>
        </triggeringPolicy>
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <pattern>
                {"level":"%level","pid":${PID},"thread":"%thread","requestId":"%X{requestId}","remoteIp":"%X{remoteIp}","traceId":"%X{X-B3-TraceId:-}","spanId":"%X{X-B3-SpanId:-}","parentSpanId":"%X{X-B3-ParentSpanId:-}","class":"%logger{50}","method":"%method","line":%line,"message":"%message","stack_trace":"%exception{10}"}%n
            </pattern>
        </encoder>
    </appender>
    <root level="INFO">
        <appender-ref ref="FILE" />
    </root>
</configuration>






























input {
  file{
    path => "C:/temp/*.logs"
    start_position => "beginning"
    sincedb_path => "NUL"
    codec => json
  }
}
filter {
  mutate {
    add_field => {"source" => "Medium"}
  }
}
output {
 file {
   path => "c:/temp/logstash_out.log"
 }
}










input {
    file {
        path => ["/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/redis.log"]
        start_position => "beginning"
        sincedb_path => "/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/sincedb.db"
        type => "redis"
	      mode => "read"
        stat_interval => "1 second"
        discover_interval => 15
        sincedb_write_interval => 15
        add_field => {
            "custom_mode" => "tail"
        }
    }

    file {
        path => ["/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/springboot.log"]
        start_position => "end"
        sincedb_path => "/Users/huan/soft/elastic-stack/logstash/logstash/pipeline.conf/multi-input/sincedb.db"
	      mode => "tail"
        type => "springboot"
    }
}



  stdout {
    codec =>  rubydebug {
      metadata => true
    }
  }

stdout {
    codec => line {
        charset => "UTF-8"
    }
  }


input {
  file {
    path => "/var/log/messages"
    tags => ["sys", "logstash_messages", "%{host}" ]
    type => syslog
  }
  file {
    path => "/var/log/cron"
    tags => ["sys", "logstash_cron", "%{host}" ]
    type => syslog
  }
  file {
    path => "/var/log/secure"
    tags => ["sys", "logstash_secure", "%{host}" ]
    type => syslog
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
      add_field => [ "received_at", "%{@timestamp}" ]
      add_field => [ "received_from", "%{host}" ]
    }
    date {
      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
    }
  }
}
output {
  if [tags][1] == "logstash_messages" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "%{tags[0]}.%{tags[1]}-%{+YYYY.MM.dd}"
    }
    file {
      path => "/var/log/logstash/%{tags[0]}/%{tags[1]}.log"
    }
  } else if [tags][1] == "logstash_cron" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "%{tags[0]}.%{tags[1]}-%{+YYYY.MM.dd}"
    }
    file {
      path => "/var/log/logstash/others/%{tags[1]}.log"
    }
  } else {
    file {
      path => "/var/log/logstash/others/other.log"
    }
  }
}



# logstash/pipeline/logstash.conf
input {
    beats {
        port => 5044
    }
}
filter {
    grok {
        match => {
            "message" => "%{JAVACLASS:exception}:\s%{GREEDYDATA:stacktrace}"
        }
        add_tag => ["stacktrace"]
    }
    grok {
        patterns_dir => ["/usr/share/logstash/pipeline/patterns"]
        match => {
            "message" => "%{TIMESTAMP_ISO8601:log_timestamp}\s*%{LOGLEVEL:log_level}\s*%{POSINT:logged_for_pid}\s*--- \[+%{NOTSPACE:logged_for_thread}+\]\s*%{JAVACLASS:logger}%{GREEDYDATA:loggercd}\s*%{MSG:log_message}"
        }
        add_tag => ["spring_boot_log"]
    }
    if [loggercd] {
        mutate {
            replace => { "logger" => "%{logger}%{loggercd}" }
            strip => ["logger"]
            remove_field => ["loggercd"]
        }
    }
    if "stacktrace" in [tags] or "spring_boot_log" in [tags] {
        mutate {
            remove_tag => ["_grokparsefailure"]
        }
    }
}
output {
    elasticsearch {
        hosts => ["${ELASTIC_HOSTS}"]
        user => "${ELASTIC_USER}"
        password => "${ELASTIC_PASSWORD}"
        index => "spring-boot-app-logs-%{+YYYY.MM.dd}"
    }
}


1) These specify the logging level (INFO)                     https://logback.qos.ch/manual/layouts.html#conversionWord
2) the appenders (where to log)  (Console)                     https://logback.qos.ch/manual/layouts.html#conversionWord
3) the format of the log messages.
%clr(%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}}){faint}
%clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint}
%clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint}
%m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}


%clr specifies a colour.
%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}} is the current date, ${VARIABLE}:- we should use the $VARIABLE environment variable for the format, if it is available, and if not, fall back to default.
${LOG_LEVEL_PATTERN:-%5p} will print the log level with right padding up to 5 characters (E.g “INFO” becomes “INFO “ but “TRACE” will not have the trailing space).
${PID:- } The environment variable $PID, if it exists. If not, space.
t	The name of the thread triggering the log message.
logger	The name of the logger (up to 39 characters), in our case this is the class name.
%m	The log message.
%n	The platform-specific line separator.
%wEx	If one exists, wEx is the stack trace of any exception, formatted using Spring Boot’s ExtendedWhitespaceThrowableProxyConverter.
%msg - outputs the actual log message.
%n - line break
%M - outputs the name of the method that the log message occurred in
%logger{36} The number inside the brackets represents the maximum length of the package + class name. If the output is longer than the specified length it will take a substring of the first character of each individual package starting from the root package until the output is below the maximum length.
%ex{short/short/any-number}






<?xml version="1.0" encoding="UTF-8"?>
<configuration>
  <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
    <encoder>
      <pattern>
        %d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger{36}.%M - %msg%n
      </pattern>
    </encoder>
  </appender>

  <root level="info">
    <appender-ref ref="STDOUT"/>
  </root>
</configuration>
logging.level.root=info
logging.pattern.console=%d{dd-MM-yyyy HH:mm:ss.SSS} %magenta([%thread]) %highlight(%-5level) %logger.%M - %msg%n

Package level logging can also be defined by simply using the package name instead of the class name in the logger tag.
logging.level.com.lankydan.service=debug
<logger name="com.lankydan.service" additivity="false" level="debug">
  <appender-ref ref="STDOUT" />
</logger>









<?xml version="1.0" encoding="UTF-8"?>
<!--
Default logback configuration provided for import
-->
<included>
	<conversionRule conversionWord="clr" converterClass="org.springframework.boot.logging.logback.ColorConverter" />
	<conversionRule conversionWord="wex" converterClass="org.springframework.boot.logging.logback.WhitespaceThrowableProxyConverter" />
	<conversionRule conversionWord="wEx" converterClass="org.springframework.boot.logging.logback.ExtendedWhitespaceThrowableProxyConverter" />
	<property name="CONSOLE_LOG_PATTERN" value="${CONSOLE_LOG_PATTERN:-%clr(%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>
	<property name="FILE_LOG_PATTERN" value="${FILE_LOG_PATTERN:-%d{${LOG_DATEFORMAT_PATTERN:-yyyy-MM-dd HH:mm:ss.SSS}} ${LOG_LEVEL_PATTERN:-%5p} ${PID:- } --- [%t] %-40.40logger{39} : %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}}"/>

	<logger name="org.apache.catalina.startup.DigesterFactory" level="ERROR"/>
	<logger name="org.apache.catalina.util.LifecycleBase" level="ERROR"/>
	<logger name="org.apache.coyote.http11.Http11NioProtocol" level="WARN"/>
	<logger name="org.apache.sshd.common.util.SecurityUtils" level="WARN"/>
	<logger name="org.apache.tomcat.util.net.NioSelectorPool" level="WARN"/>
	<logger name="org.eclipse.jetty.util.component.AbstractLifeCycle" level="ERROR"/>
	<logger name="org.hibernate.validator.internal.util.Version" level="WARN"/>
	<logger name="org.springframework.boot.actuate.endpoint.jmx" level="WARN"/>
</included>


(Logback is a logging framework for Java, and a successor to the old log4j.)
How to customise Spring Boot’s logging configuration
You can customise the default logging configuration in one of these ways:
1) Setting some Spring Boot logging properties – which can go in your application.properties, application.yml, or as environment variables in your shell
    # Examples for Spring Boot 2.x
    logging.file.path=.             # write logs to the current directory
    logging.file.path=/home/logs    # write logs to /home/logs
    logging.file.path=/mnt/logdir   # write logs to /mnt/logdir
    # Example for Spring Boot 2.x
    logging.file.name=myapp.log

    Just convert each property to upper-case, and change dots to underscores.
2) Adding a logback.xml onto the classpath, which Spring Boot will detect, and use to configure Logback
3) Adding a logback-spring.xml onto the classpath, which Spring Boot will detect, and use to configure Logback





How the magic works
How does Spring detect Logback and abstract it away from us?

1) Looking around the source code, I found these relevant clues (hunted down in Spring Boot v2.6.6):
2) Logback should appear on the classpath in most Spring Boot projects because… spring-boot-starter-web includes the dependency spring-boot-starter-logging, which includes logback as a dependency.
3) Spring’s logging system detects Logback because… The abstract class LoggingSystem contains some code which detects and returns the logging system that’s currently in use. It checks which classes are present in the classloader.
4) Spring knows how to write logs with Logback because… There’s a subclass of LoggingSystem, called LogbackLoggingSystem, which basically configures and works with Logback.
We don’t need to configure Logback ourselves because… Inside DefaultLogbackConfiguration, Spring Boot does some programmatic auto-configuration of Logback for us, so we don’t need to configure it manually.

Push log entries from a Spring Boot application
By default, Spring Boot uses Logback for logging which can be fine-tuned by creating a logback-spring.xml configuration:

<?xml version="1.0" encoding="UTF-8"?>
<configuration>
    <include resource="org/springframework/boot/logging/logback/defaults.xml"/>
    <include resource="org/springframework/boot/logging/logback/console-appender.xml"/>

    <appender name="logstashNetworkAppender" class="net.logstash.logback.appender.LogstashTcpSocketAppender">
        <destination>localhost:5000</destination>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
        </encoder>
        <keepAliveDuration>5 minutes</keepAliveDuration>
    </appender>

    <root level="INFO">
        <appender-ref ref="logstashNetworkAppender"/>
        <appender-ref ref="CONSOLE"/>
    </root>
</configuration>
This snippet adds the LogstashTcpSocketAppender which sends the log entries in JSON format to the Logstash node and it also keeps using the default Console Appender.



logging:
  level:
    root: INFO
    com.abc.demo: DEBUG
  pattern:
    console: "%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%8.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %X{REQUEST_ID} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"

"%clr(%d{yyyy-MM-dd HH:mm:ss.SSS}){faint} %clr(${LOG_LEVEL_PATTERN:-%5p}) %clr(${PID:- }){magenta} %clr(---){faint} %clr([%15.15t]){faint} %clr(%-40.40logger{39}){cyan} %clr(:){faint} %m%n${LOG_EXCEPTION_CONVERSION_WORD:-%wEx}"
2020-04-23 21:09:44.107 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP GET "/posts" , parameters={requestId=1234}, body=, remote_address=0:0:0:0:0:0:0:1
2020-04-23 21:09:44.110 - INFO 3240 --- ApiLoggingFilter: 1234 HTTP RESPONSE [{"id":1,"title":"Spring Boot","body":"All about Spring boot microservice"},{"id":2,"title":"Java","body":"Learn Streams in Java"},{"id":3,"title":"JavaScript","body":"Whats new in ES6"}]







