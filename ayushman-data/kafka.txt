https://www.metricstream.com/blog/improve-platform-resilience-kafka-based-systems.html
https://www.clairvoyant.ai/blog/unleash-kafka-producers-architecture-and-internal-workings





By default, the Spring Boot configuration will disable the following:
-------------------------------------------------------------------------
MapperFeature.DEFAULT_VIEW_INCLUSION
DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES
SerializationFeature.WRITE_DATES_AS_TIMESTAMPS

    @Bean
    @Primary
    public ObjectMapper customJson(){
        return new Jackson2ObjectMapperBuilder()
                .indentOutput(true)
                .serializationInclusion(JsonInclude.Include.NON_NULL)
                .propertyNamingStrategy(PropertyNamingStrategy.UPPER_CAMEL_CASE)
                .build();
    }

PdFundManagementService
va_Fund_tran  VA_FUND_TRANSFER

public interface JsonMapperService {//https://octoperf.com/blog/2018/02/08/spring-autowiring/#collection-autowiring
  String toJson(Object instance) throws IOException;
  <T> T fromJson(String json, Class<T> clazz) throws IOException;
}

This service provides two methods:
-------------------------------------------
A) serializing an object instance to Json,
B) deserializing a json to an object instance.

    protected String toJson(Object obj) throws JsonProcessingException {
        ObjectMapper objectMapper = new ObjectMapper();
        return objectMapper.writeValueAsString(obj);
    }

    protected <T> T fromJson(String json, Class<T> clazz) throws JsonParseException, JsonMappingException, IOException {
        ObjectMapper objectMapper = new ObjectMapper();
        return objectMapper.readValue(json, clazz);
    }


@Component
public class JacksonConfig implements Jackson2ObjectMapperBuilderCustomizer, Ordered {

    /** Default Date Time Format */
    private final String dateTimeFormat = "yyyy-MM-dd HH:mm:ss";
    /** Default date format */
    private final String dateFormat = "yyyy-MM-dd";
    /** Default time format */
    private final String timeFormat = "HH:mm:ss";

    @Override
    public void customize(Jackson2ObjectMapperBuilder builder) {
        // Set the format of serialization and deserialization of the java.util.Date.
        builder.simpleDateFormat(dateTimeFormat);

        // JSR 310 Date Time Processing
        JavaTimeModule javaTimeModule = new JavaTimeModule();

        DateTimeFormatter dateTimeFormatter = DateTimeFormatter.ofPattern(dateTimeFormat);
        javaTimeModule.addSerializer(LocalDateTime.class, new LocalDateTimeSerializer(dateTimeFormatter));
        javaTimeModule.addDeserializer(LocalDateTime.class, new LocalDateTimeDeserializer(dateTimeFormatter));

        DateTimeFormatter dateFormatter = DateTimeFormatter.ofPattern(dateFormat);
        javaTimeModule.addSerializer(LocalDate.class, new LocalDateSerializer(dateFormatter));
        javaTimeModule.addDeserializer(LocalDate.class, new LocalDateDeserializer(dateFormatter));

        DateTimeFormatter timeFormatter = DateTimeFormatter.ofPattern(timeFormat);
        javaTimeModule.addSerializer(LocalTime.class, new LocalTimeSerializer(timeFormatter));
        javaTimeModule.addDeserializer(LocalTime.class, new LocalTimeDeserializer(timeFormatter));

        builder.modules(javaTimeModule);

transaction.setTitle("Paid towards"+" "+operatorName+" recharge - "+mobileNumber);

transactionListView.setInfo1(operatorName+" Recharge of "+ mobileNumber);

OUT-PUT
----------------------------------
{Task 1=In_Progress, Task 2=Done, Task 3=Planned}
{
  "Task 1" : "In_Progress",
  "Task 2" : "Done",
  "Task 3" : "Planned"
}
2) Global Pretty-Print Setting
----------------------------------------
ObjectMapper mapper = new ObjectMapper().enable(SerializationFeature.INDENT_OUTPUT);

3) Pretty-Printing Jackson with Spring Boot
----------------------------------------------
spring.jackson.serialization.indent_output=true
spring.jackson.default-property-inclusion=always, non_null, non_absent, non_default, non_empty
OR 
spring:
  jackson:
    serialization:
      INDENT_OUTPUT: true


What’s the need
-----------------
A dead letter queue is like a graveyard of messages. All the messages that couldn’t be processed will be consumed by a dead letter queue. The advantage of having this queue is that it allows the developer to get the messages that could not be delivered and take fallback actions appropriately.

Implementation
-------------------------
Implementation of a dead letter queue is pretty straight forward. Just having another Kafka consumer to a new topic where all dead messages will be sent. But the key concept is to understand when to push messages to the dead letter queue.

The following cases will help us understand:
-----------------------------------------------
A) Runtime Exceptions: Catch the exception and push the message into the dead letter queue to understand why the exception occured.
B) Retry Count Exhausted: If retryCount reaches 0, then manual processing of the message may be required by the operations team.
C) Large processing time: If a message takes a very long time to be processed, incurring delays to other messages.



Duplicate Messages:
----------------------------
Duplicate messages can occur in the scenario where:
A Producer attempts to write a message to a topic partition.
The broker does not acknowledge the write due to some transient failure scenario.
The Producer should retry as it does not know whether the write succeeded or not.
If the Producer is not idempotent and the original write did succeed then the message would be duplicated.


// create safe Producer
properties.setProperty(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, "true");
properties.setProperty(ProducerConfig.ACKS_CONFIG, "all");
properties.setProperty(ProducerConfig.RETRIES_CONFIG, Integer.toString(Integer.MAX_VALUE));
properties.setProperty(ProducerConfig.MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION, "5"); // kafka 2.0 >= 1.1 so we can keep this as 5. Use 1 otherwise.


https://thepracticaldeveloper.com/spring-boot-kafka-config/
https://arnoldgalovics.com/reliable-kafka-messaging-spring-boot/


zookeeper-server-start.bat ..\..\config\zookeeper.properties
kafka-server-start.bat ..\..\config\server.properties
kafka-topics.bat --create --topic my-topic --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3
//[kafka directory]\bin\windows\kafka-topics.bat --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic [topic name]
kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic
kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic --from-beginning
//kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic [topic name] --from-beginning


ListenableFuture<SendResult<String, String>> future = this.kafkaTemplate.send(TOPIC, message);
future.addCallback(new ListenableFutureCallback<>() {
    @Override
    public void onFailure(Throwable ex) {
    logger.info("Unable to send message=[ {} ] due to : {}", message, ex.getMessage());
    }

    @Override
    public void onSuccess(SendResult<String, String> result) {
    logger.info("Sent message=[ {} ] with offset=[ {} ]", message, result.getRecordMetadata().offset());
    }
});


@KafkaListener(id = "normal-topic-consumer", groupId = "normal-topic-group", topics = "normal-topic")
public void consume(ConsumerRecord<?, ?> consumerRecord, Acknowledgment ack) {
    log.info("Getting consumer record key: '" + consumerRecord.key() + "', value: '" + consumerRecord.value() + "', partition: " + consumerRecord.partition() + " and offset: " + consumerRecord.offset() + " at " + new Date(consumerRecord.timestamp()));
    String json = consumerRecord.value().toString();
    log.info("Consuming normal message {}", json);
    ack.acknowledge();
}








ERROR
When the application is running in the ERROR log, the  WARN, INFO, DEBUG and TRACE type logs are not logged.

WARN
When the application log level is WARN, this will log both WARN as well as ERROR type logs into the logging system.

INFO
When the log level for the entire application is set to INFO, then both the WARN and ERROR logs are also logged into the logging system.

DEBUG
Enabling DEBUG level at the application level will cause INFO, WARN and ERROR logs to be also sent to the logging system.
 
TRACE
Enabling TRACE will enable all the subsequent log levels and is not at all recommended in the production environment.