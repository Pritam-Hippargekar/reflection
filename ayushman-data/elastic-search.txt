elastic-search
================https://iridakos.com/programming/2019/05/02/add-update-delete-elasticsearch-nested-objects
https://juejin.cn/post/7080726607043756045
https://iridakos.com/programming/2019/05/02/add-update-delete-elasticsearch-nested-objects
https://logz.io/blog/elasticsearch-mapping/
https://iridakos.com/programming/2019/05/02/add-update-delete-elasticsearch-nested-objects
https://sergiiblog.com/spring-boot-elasticsearch-model-data-layer/

https://blogs.perficient.com/2022/08/22/elasticsearch-java-api-client-springboot/


discovery.seed_hosts:[] ==== setting Provides a list of master-eligible nodes in the cluster.
cluster.initial_master_nodes:[] === setting defines the initial set of master-eligible nodes.
The value of this setting MUST match the value of node.name

@Document(indexName = "goods",type = "_doc",shards = 3, replicas = 1)
https://kifarunix.com/how-to-enable-basic-authentication-on-elk-stack/


InputStream input = this.getClass()
    .getResourceAsStream("some-index.json"); 
CreateIndexRequest req = CreateIndexRequest.of(b -> b
    .index("some-index")
    .withJson(input) 
);
boolean created = client.indices().create(req).acknowledged();




@Document(indexName = "goodsIndex",type = "goodsType",shards = 3, replicas = 1)
public class Sale {
   @Id
   public String orderNo;
   @Field(type = FieldType.Text, analyzer = "ik_max_word")
   public String title;
   public Date date;
   public Double amount;
   @Field(type = FieldType.Keyword)
   public String reference;
   @Field(type = FieldType.Nested,name = "line_items")
   public List<LineItem> lineItems = new ArrayList();
}

public class LineItem {
   public String itemCode;
   public String itemName;
   public Double qty;
   public Double price;
}




mapping_test.json
---------------------------https://hackmd.io/@obj0/SkUf0ixio-
{
    "properties": {
        "brandName": {
            "type": "keyword"
        }, 
        "categoryName": {
            "type": "keyword"
        }, 
        "createTime": {
            "type": "date", 
            "format": "yyyy-MM-dd HH:mm:ss"
        }, 
        "id": {
            "type": "long"
        }, 
        "price": {
            "type": "double"
        }, 
        "saleNum": {
            "type": "integer"
        }, 
        "status": {
            "type": "integer"
        }, 
        "stock": {
            "type": "integer"
        }, 
        "spec": {
            "type": "text", 
            "analyzer": "ik_max_word", 
            "search_analyzer": "ik_smart"
        }, 
        "title": {
            "type": "text", 
            "analyzer": "ik_max_word", 
            "search_analyzer": "ik_smart"
        }
    }
}

@Service
public class IndexTestServiceImpl implements IndexTestService {
    
    @value("classpath:json/mapping_test.json")
    private resoure mappingTest;

    @Autowired
    RestHighLevelClient restHighLevelClient;
    
    private String shardNumName = "number_of_shards";
    private String replicaNumName = "number_of_replicas";
    private String index = "goodsIndex"

    @Override
    public boolean indexCreate() throws Exception {
        CreateIndexRequest indexRequest = new CreateIndexRequest(index);
        
        indexRequest.settings(Settings.builder().put(shardNumName, 3).put(replicaNumName, 1));
        
        String mappingJson = IOUtils.toString(mappingTest.getInputStream(), Charset.forName("UTF-8"));
        indexRequest.mapping(mappingJson, XContentType.JSON);

        IndicesClient indicesClient = restHighLevelClient.indices();
        CreateIndexResponse response = indicesClient.create(indexRequest, RequestOptions.DEFAULT);

        return response.isAcknowledged();
    }
}  



@Document(indexName = "sample", type = "employee")
public class Employee {

    @Id
    private Long id;
    @Field(type = FieldType.Object)
    private Organization organization;
    @Field(type = FieldType.Object)
    private Department department;
    private String name;
    private int age;
    private String position;
	
    // Getters and Setters ...

}


Map<String, Property> properties = new HashMap<>();
Property keywordProperty = Property.of(pb -> pb.keyword(builder -> builder));
properties.put(Fields.Id.fieldName, keywordProperty);
properties.put(Fields.ContextPath.fieldName, keywordProperty);
properties.put(Fields.Vhost.fieldName, keywordProperty);
properties.put(Fields.LastNodeUpdated.fieldName, keywordProperty);

Property dateProperty = 
    Property.of(pb -> pb.date(builder -> builder.format("epoch_millis")));
properties.put(Fields.EpochCreated.fieldName, dateProperty);
properties.put(Fields.EpochAccessed.fieldName, dateProperty);
properties.put(Fields.EpochLastAccessed.fieldName, dateProperty);
properties.put(Fields.EpochCookieSet.fieldName, dateProperty);
properties.put(Fields.EpochExpiry.fieldName, dateProperty);

properties.put(Fields.SessionData.fieldName,
    Property.of(pb -> pb.binary(builder -> builder)));
properties.put(Fields.MaxInactiveMs.fieldName,
    Property.of(pb -> pb.long_(nb -> nb)));

client.indices().putMapping(b -> b.index(INDEX).properties(properties));

गुढीपाडव्याच्या हार्दिक शुभेच्छा
2023 गुढी पाडव्याच्या हार्दिक शुभेच्छा

// check for correct mapping and bail if it does not work
final GetMappingResponse mapping = client.indices().getMapping(b -> b.index(INDEX));
final Map<String, Property> properties = mapping.get(INDEX).mappings().properties();
// this ensures the right types are set
properties.get(Fields.Id.fieldName).keyword();
properties.get(Fields.ContextPath.fieldName).keyword();
properties.get(Fields.Vhost.fieldName).keyword();
properties.get(Fields.LastNodeUpdated.fieldName).keyword();
properties.get(Fields.SessionData.fieldName).binary();
properties.get(Fields.MaxInactiveMs.fieldName).long_();
properties.get(Fields.EpochCreated.fieldName).date();
properties.get(Fields.EpochAccessed.fieldName).date();
properties.get(Fields.EpochLastAccessed.fieldName).date();
properties.get(Fields.EpochCookieSet.fieldName).date();
properties.get(Fields.EpochExpiry.fieldName).date();




ElasticsearchClient client = new ElasticsearchClient(
                new RestClientTransport(
                    RestClient.builder(new HttpHost("localhost", 9200, "http")).build(),
                    new JacksonJsonpMapper()));

String mappingPath = "yourPath/yourJsonFile.json";
 
JsonpMapper mapper = client._transport().jsonpMapper();
JsonParser parser = mapper.jsonProvider()
            .createParser(new StringReader(
                Files.toString(new ClassPathResource(mappingPath).getFile(), Charsets.UTF_8)));

client.indices()
    .create(createIndexRequest -> createIndexRequest.index(indexName)
        .mappings(TypeMapping._DESERIALIZER.deserialize(parser, mapper)));






CreateIndexRequest request = new CreateIndexRequest(esProperties.getTestIndexName());
        request.settings(Settings.builder()
                .put("index.number_of_shards", 1)
                .put("index.number_of_replicas", 0));
ActionListener<CreateIndexResponse> listener = new ActionListener<CreateIndexResponse>() {
    @Override
    public void onResponse(CreateIndexResponse createIndexResponse) {
        System.out.println("성공!!");
    }

    @Override
    public void onFailure(Exception e) {
        log.error("인덱스 생성에 실패했습니다. 원인 : " + e.getMessage());
    }
};
client.indices().createAsync(request, RequestOptions.DEFAULT, listener);




If you are a Java backend developer and use springboot in your project, you can introduce starter
----------------------------------------------------------------------------------------------------
<dependency>
    <groupId>org.springframework.boot</groupId>
    <artifactId>spring-boot-starter-data-elasticsearch</artifactId>
    <version>${spring-boot.version}</version>
</dependency>

or use the client directly
-------------------------------------------
<dependency>
    <groupId>co.elastic.clients</groupId>
    <artifactId>elasticsearch-java</artifactId>
    <version>8.1.2</version>
</dependency>

Since the elasticsearch official website client is consistent with the elasticsearch version, and the version supported by springboot lags behind the official version,
 please pay attention to the dependent package version when introducing the jar package. For example, jakarta.json-api, the version of springboot is 1.1.6, 
 and the version that elasticsearch-java depends on is 2.0.1. 
Then the version of jakarta.json-api will be overwritten by the version of the parent package, resulting in the class jakarta.json.spi.JsonProvider not being found
The specific method is this
-------------------------------
<dependency>
    <groupId>co.elastic.clients</groupId>
    <artifactId>elasticsearch-java</artifactId>
    <version>8.1.3</version>
    <exclusions>
        <exclusion>
            <artifactId>jakarta.json-api</artifactId>
            <groupId>jakarta.json</groupId>
        </exclusion>
    </exclusions>
</dependency>
<dependency>
    <artifactId>jakarta.json-api</artifactId>
    <groupId>jakarta.json</groupId>
    <version>2.0.1</version>
</dependency>

The Java client is implemented based on the builder pattern, and lamda expressions can be used throughout.
    private void createIndex(String index) throws IOException {

        CreateIndexRequest.Builder indexBuilder = new CreateIndexRequest.Builder();
        indexBuilder.index(index);
        TypeMapping.Builder tmBuilder = new TypeMapping.Builder();
        tmBuilder.properties("vec", new Property.Builder().denseVector(builder -> builder.index(true).dims(64).similarity("dot_product")
                .indexOptions(opBuilder -> opBuilder.type("hnsw").m(16).efConstruction(100))).build());
        tmBuilder.properties("id", new Property.Builder().long_(pb -> pb.index(false)).build());
        TypeMapping typeMapping = tmBuilder.build();
        indexBuilder.mappings(typeMapping);
        CreateIndexResponse createIndexResponse = esClient.indices().create(indexBuilder.build());

        String resIndex = createIndexResponse.index();
        Boolean acknowledged = createIndexResponse.acknowledged();
        boolean b = createIndexResponse.shardsAcknowledged();
        log.info("index create response for {}, acknowledged= {}, shardsAcknowledged= {}", resIndex, acknowledged, b);
    }

keyword	Not analyzed text
text		Analyzed text


"mappings": {
  "index_type_name": {
    "properties": {
        "description" : {
          "type" : "text",
          "fields" : {
            "keyword" : {"type" : "keyword", "ignore_above" : 256 }
          }
        }, 
        "speciality": {
          "type": "object",
          "properties": {
            "field_1": { "type": "text", "analyzer": "standard"},
            "field_2": { "type": "keyword"}
          }
        },
        "publisher": {
          "type": "nested",
          "include_in_root": true
          "properties": {
            "field_1": { "type": "text", "analyzer": "standard"},
            "field_2": { "type": "keyword"}
          }
        },
        "user" : { "type" : "keyword", "index":"no" },
        "count": {"type": "long"},
        "active": { "type": "boolean" },
        "location": { "type": "geo_point" },
        "post_date": { "type": "date", "format": "yyyy-MM-dd"},
        "consultation_fee": { "type": "float" },
        "message" : { "type" : "text" },
    }
  }
}



============================START OBJECT========================================

{
  "title": "Machine Learning",
  "author": {
    "age": 30,
    "name": {
      "first": "Elie",
      "last": "John"
    }
  }
}
it become
{
  "title": "Machine Learning",
  "author.age": 30,
  "author.name.first": "Elie",
  "author.name.last": "John"
}

=====As long as the root object and the inner object have a one-to-one relationship, the inner object mapping discussed above will work. 
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text"
      },
      "author": {
        "type": "object",///Since the object is a default value, it is not necessary to explicitly assign the field type to the object.
        "properties": {
          "age": {
            "type": "float"
          },
          "name": {
            "type": "object",///Since the object is a default value, it is not necessary to explicitly assign the field type to the object.
            "properties": {
              "first": {
                "type": "text"
              },
              "last": {
                "type": "text"
              }
            }
          }
        }
      }
    }
  }
}

============================END OBJECT========================================


============================START NESTED========================https://opster.com/guides/elasticsearch/data-architecture/how-to-model-relationships-between-elasticsearch-documents-using-object/================
{
  "title": "Machine Learning",
  "author": [
    {
      "first_name": "John",
      "last_name": "Stefan"
    },
    {
      "first_name": "Sandy",
      "last_name": "Naily"
    }
  ]
}

{
  "mappings": {
    "properties": {
      "title": {
        "type": "text"
      },
      "author": {
        "type": "nested",
 //       "include_in_root": true,
        "properties": {
          "age": {
            "type": "float"
          },
          "name": {
            "type": "nested",
//            "include_in_parent": true,
            "properties": {
              "first": {
                "type": "text"
              },
              "last": {
                "type": "text"
              }
            }
          }
        }
      }
    }
  }
}

{
  "title": "Machine Learning",
  "author": {
    "age": 30,
    "name":[ 
      {
        "first": "Elie",
        "last": "John"
     }
    ]
  }
}
============================END NESTED========================================


BigDecimal (The compare method is the heart and soul of the Comparator interface)
--------------------------
BigDecimal is zero means that it can store 0, 0.0 or 0.00 etc values.
So you want to check Integer is zero and the precision is also zero.
BigDecimal object has enum constant BigDecimal.ZERO to represent 0 value with zero scales.
use BigDecimal.ZERO to check object is zero or not.

equals: It does not use a scale for equality test
compareTo: method checks zero value and zero scale value and returns 0. It is null-safe and more readable.

equals is not considered the scale zero
compareTo is null safe, readable, and considered scale zero
signum is not readable, not null safe, and considered scale zero

When (this BigDecimal) == (BigDecimal ob) [t1 == t2], it returns 0.(0 represents equal objects)
When (this BigDecimal) < (BigDecimal ob) [t1 < t2], it returns -1.(a negative number represents less than)
When (this BigDecimal) > (BigDecimal ob) [t1 > t2], it returns 1.(1 represents greater than)
HashSet uses equals to check duplicates while TreeSet uses compareTo to check duplicates.
CompareTo() must throw NullPointerException if current object get compared to null object as opposed to equals() which return false on such scenario.
Use compareTo() instead of equals() to compare numeric values since equals() takes scale into consideration
compares two objects of “BigDecimal” type and when both the objects are same in value but different in terms of the scale then it is treated to be equal.


Comparator<Transaction> TIME_SORT = (Transaction t1, Transaction t2) -> t1.date().compareTo(t2.date());
Sort using Collections class.
Collections.sort(transactions, TIME_SORT);

Sort using sort method in List interface.
transactions.sort(TIME_SORT);




Example: Calculate 10% of 800.
10/100 * 80 = X
BigDecimal serviceCharge = serviceChargesDetails.getServiceCharge().divide(new BigDecimal(100)).multiply(txnAmount).setScale(4, BigDecimal.ROUND_HALF_UP);

Example: Calculate 96.163 is what % of 3.7
(96.163 * 100) / 3.7


public BigDecimal calculateCommission(BigDecimal Pay1_Operator_Commisssion,BigDecimal txnAmount){
  return pay1_operator_commisssion.divide(new BigDecimal(100)).multiply(txnAmount).setScale(2, BigDecimal.ROUND_HALF_UP);
}

    public BigDecimal getCommissionGst(BigDecimal txnAmount) {
        if (txnAmount.compareTo(BigDecimal.ZERO) > 0) {
            ParameterManagement parameterManagement = parameterManagementRepository.findByName(ParameterManagementEnum.GST_RATE.name());
            BigDecimal gstPercentage = new BigDecimal(parameterManagement.getValue());
            return gstPercentage.divide(new BigDecimal(100)).multiply(txnAmount).setScale(2, BigDecimal.ROUND_HALF_UP);
        }
        return BigDecimal.ZERO;
    }

Prior to Java 8 Comparator<T> interface had two methods 
-----------------------------------
boolean equals(Object obj)
int compare(T o1, T o2)


due date should be before current date.



mobile circle operator

PAY1_OPERATOR_COMMISSION
    <changeSet id="2022110812121234" author="jhipster">
        <sql>
            INSERT INTO parameter_management (name, value, data_type, comments, unit) VALUES ('PAY1_OPERATOR_COMMISSION', 
    '{"data": [{ "operator_code": "2", "operator_name": "Airtel", "commission": "0" },
        { "operator_code": "3", "operator_name": "BSNL", "commission": "0" }, { "operator_code": "30", "operator_name": "MTNL", "commission": "0" },
        { "operator_code": "83", "operator_name": "Reliance Jio", "commission": "0" }, { "operator_code": "15", "operator_name": "Vodafone Idea", "commission": "0" }
    ]}' , 'STRING', 'PAY1 OPERATOR COMMISSION', 'STRING');
        </sql>
    </changeSet>

employee hired — NewObject,
employee fired — ObjectRemoved,
salary change — ValueChange,
boss change — ReferenceChange.



my_index
PUT x3/x4/_mapping
{
   "x4": {
      "properties": {
         "name": {
            "type": "string"
         },
         "car": {
            "type": "nested",
            "properties": {
               "make": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "model": {
                  "type": "string",
                  "index": "not_analyzed"
               },
               "colors": {
                  "type": "string",
                  "index": "not_analyzed"
               }
            }
         }
      }
   }
}
PUT /x3/x4/1
{
  "name": "Zach",
  "car": [
    {
      "make": "Saturn",
      "model": "SL",
      "colors": ["Red", "Blue","Green" ]
    },
    {
      "make": "Saturn",
      "model": "Imprezza",
      "colors": ["Pink", "Green"]
    }
  ]
}












SELECT Customers.CustomerID, Customers.Name, Count(Sales.SalesID)
FROM Customers
   INNER JOIN Sales
   ON Customers.CustomerID = Sales.CustomerID
WHERE Sales.LastSaleDate BETWEEN #1/1/2016# AND #12/31/2016#
GROUP BY Customers.CustomerID, Customers.Name
HAVING Count(Sales.SalesID) > 5




WITH salaries_ranks AS (
 SELECT e.first_name,
 e.last_name,
 d.department_name,
 salary,
 ROW_NUMBER() OVER (PARTITION BY d.id ORDER BY salary DESC) AS salary_rank
 FROM department d JOIN employee e ON d.id = e.department_id
)
 
SELECT *
FROM salaries_ranks
WHERE salary_rank = 3;

select * FROM (
  select EmployeeID, Salary
  ,rank() over (partition by deptno order by Salary DESC) ranking
  from Employee
)
WHERE ranking = N;


Match Query = will get analyzed into terms first
Term Query = will not get analyzed into terms first
 
“text” : full-text and relevancy search in documents  ex :  bodies of e-mails or product descriptions
Use text field data type if:
------------------------------
You want to create an autocomplete
You want to create a search system


“keyword” : exact-value search for sorting, aggregation and filtering documents ex : email addresses, hostnames, status codes, zip codes, or tags.
Use keyword field data type if:
----------------------------------
You want an exact match query
You want to make Elasticsearch function like other databases
You want to use it for wildcard query


VALUES('Hardware') Computers, Mobile phones, Laptop
VALUES('Software')  software like Word, Excel, PowerPoint,//operating systems like linux and Windows


VALUES('Laptop 2', '1000.00', 2, 'good', 1, '2016-06-10', 1);
VALUES('Laptop 3', '200.00', 5, 'good', 1, '2015-06-15', 1);
VALUES('Laptop 4', '500.00', 8, 'good', 0, '2015-06-24', 1);
VALUES('Computer 1', '560.00', 10, 'good', 0, '2015-02-25', 2);
VALUES('Computer 2', '520.00', 4, 'good', 0, '2015-06-28', 2);
VALUES('Computer 3', '720.00', 5, 'good', 1, '2015-08-25', 2);
VALUES('Laptop 1', '110.00', 19, 'good', 0, '2016-07-12', 2);
VALUES('Mobile 1', '222.00', 5, 'good', 1, '2015-03-16', 3);
VALUES('Mobile 2', '1000.00', 4, 'good', 1, '2015-06-27', 3);
VALUES('Mobile 3', '1000.00', 4, 'good', 1, '2015-06-27', 3);


public class Category{
  private Long categoryId;
	private String categoryName;
  private Set<Product> products;
}

public class Product{
  private Long productId;
	private String productName;
	private BigDecimal price;
	private int quantity;
	private String description;
	private boolean active;
	private Date createdAt;
  private Date updatedAt;
  private GeoLocation location;
}

class GeoLocation {  //https://www.bytefish.de/blog/elasticsearch_java.html
  private double lat;
  private double lon;

  public GeoLocation() {}

  public GeoLocation(double lat, double lon) {
      this.lat = lat;
      this.lon = lon;
  }

  @Override
  public String toString() {
      return "CustomGeoPoint{" + "lat=" + lat + ", lon=" + lon + '}';
  }
}


    "mappings": {
        "category": {
            "properties": {
                "categoryId":  { "type": "long" },
                "categoryName":{ "type": "text" },
                "products": {
                    "type": "nested",
                    "properties": {
                        "productId":    { "type": "long"  },
                        "productName":  { "type": "text"  },
                        "price":        { "type": "float" },
                        "stocks":       { "type": "integer" },
                        "description":  { "type": "text" },
                        "active":       { "type": "boolean" },
                        "createdAt":    { "type": "date" },
                        "updatedAt":    { "type": "date" },
                        "location":     { "type" : "geo_point" },
                        "productName": {
                            "type": "text",
                            "fields": {
                                "keyword": {
                                    "type": "keyword",
                                    "ignore_above": 256
                                }
                            }
                        }
                    }
                }
            }
        }
    }


 "mappings": {
      "product": {
          "properties": {
            "productId":    { "type": "long"  },
            "productName":  { "type": "text"  },
            "price":        { "type": "float" },
            "stocks":     { "type": "integer" },
            "description":  { "type": "text" },
            "active":       { "type": "boolean" },
            "createdAt":    { "type": "date" },
            "updatedAt":    { "type": "date" },
            "location":    { "type" : "geo_point" },
            "productName": {
              "type": "text",
              "fields": {
                "keyword": { "type": "keyword", "ignore_above": 256 }
              }
            },
            "category": {
                "type": "nested",
                "properties": {
                    "categoryId":  { "type": "long" },
                    "categoryName":{ "type": "text" },
                }
            }
          }
      }
  }


"location": {
      "lat": 49.0090639,
      "lon": 8.4028365
    },
"distance": 1.2335132790824628

"location":{
    "properties":{
        "lat":{
            "type":"float"
        },
        "lon":{
            "type":"float"
        }
    }
}


"script" : {
    "inline": "ctx._source.counter = ctx._source.value ? ctx._source.counter += 1 : 1"
}


"bookId": {
                    "type": "keyword"
                },
                "message": {
                    "type": "text"
                },
                "timestamp": {
                    "format": "epoch_millis",
                    "type": "date"
                }




public void removeFromAgenda(User user, String sessionId) throws IOException {
    String script = """
        if (ctx._source.agenda == null) { ctx.op = 'none' } else if (!ctx._source.agenda.removeIf(id -> id.equals(params.sessionId))) { ctx.op = 'none'; }
    """;

    client.update(b -> b.index(INDEX).id(user.username())
        .script(s -> s
            .inline(sb -> sb
                .source(script)
                .lang("painless")
                .params(Map.of("sessionId", JsonData.of(sessionId)))
            )
        )
        .retryOnConflict(3),
        User.class);
    }



https://atetux.com/how-to-install-elastic-stack-8-on-debian-11

for the network there are 3 options
----------------------------------------------
0.0.0.0 which will make your configuration accept all network connections
127.0.0.1 which is local host, and this is actually the default
You can choose to bind the network. host with your local Ip address


https://medium.com/devops-dudes/elasticsearch-8-x-deployment-ac990b9e4c56
https://unixwise.xyz/wordpress/2022/07/install-elastic-elk832-onto-ubuntu20/
Step 1: su to root user
----------------
sudo -i

Step 2: update distro
------------------------
apt update -y && apt upgrade -y

Step 3: Add elastic repository
------------------------------------
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
or 
curl -fsSL https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -

Step 4: update APT repo
--------------------------
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | tee /etc/apt/sources.list.d/elastic-8.x.list
or 
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list

Step 5: refresh APT repo
-------------------------
apt update -y

Step 6: install elasticsearch
------------------------------
apt install elasticsearch

Add path to environment
----------------------------
echo "export PATH=/usr/share/elasticsearch/bin/:$PATH" >> /root/.bashrc

Step 7: Register Elasticsearch as daemon, and fire up
--------------------------------------------------------
systemctl daemon-reload
systemctl enable elasticsearch
systemctl start elasticsearch

Step 8: Health check – service level
----------------------------------------
systemctl status elasticsearch
or 
service elasticsearch status

Step 9: Health check – web output
------------------------------------------
curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic https://localhost:9200
curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic:PASSWORD https://localhost:9200
note: by default, Elasticsearch is opening 192.168.0.1:9200 for service




https://www.skyer9.pe.kr/wordpress/?p=5655   imp https://www.instaclustr.com/support/documentation/elasticsearch/using-elasticsearch/connecting-to-elasticsearch-with-java/

Setting up elasticsearch (8.5.2) ...
--------------------------- Security autoconfiguration information ------------------------------

Authentication and authorization are enabled.
TLS for the transport and HTTP layers is enabled and configured.

The generated password for the elastic built-in superuser is : DkHFDl=j=x2wbmhunCCT

If this node should join an existing cluster, you can reconfigure this with
'/usr/share/elasticsearch/bin/elasticsearch-reconfigure-node --enrollment-token <token-here>'
after creating an enrollment token on your existing cluster.

You can complete the following actions at any time:

Reset the password of the elastic built-in superuser with 
'/usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic'.

Generate an enrollment token for Kibana instances with 
 '/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana'.

Generate an enrollment token for Elasticsearch nodes with 
'/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node'.

-------------------------------------------------------------------------------------------------
### NOT starting on installation, please execute the following statements to configure elasticsearch service to start automatically using systemd
 sudo systemctl daemon-reload
 sudo systemctl enable elasticsearch.service
### You can start elasticsearch service by executing
 sudo systemctl start elasticsearch.service




https://itnixpro.com/install-elk-stack-8-on-ubuntu/  ===== https://itnixpro.com/install-elk-stack-8-on-ubuntu/
 If you want to view Elasticsearch service logging
 sudo journalctl -u elasticsearch
 sudo journalctl -u elasticsearch -f



client.incides.create({
  index: "developers",
  type: "developer",
  body: {
    mappings: {
      developer: {
        properties: {
          name: { type: "text" },
          skills: {
            type: "nested",
            properties: {
              language: { type: "keyword" },
              level: { type: "keyword"}
            }
          }
        }
      }
    }
  }
})


{
  "script": {
    "lang": "painless",
    "source": """
            for (int i = 0; i < ctx._source.employees.length; ++i) {
              if(ctx._source.employees[i].firstname == params.emp_name) {
                ctx._source.employees[i].age = 35;
              }
            }
""",
    "params": {
      "emp_name": "Garrett"
    }
  }
}
https://codecurated.com/blog/how-to-connect-java-with-elasticsearch/
https://juejin.cn/post/7070469367456071717  ==  IMP
How to manage nested objects in Elasticsearch documents
------------------------------------------------------------
https://iridakos.com/programming/2019/05/02/add-update-delete-elasticsearch-nested-objects

{
  "mappings": {
    "blog": {
      "properties": {
        "productId": { "type": "integer" },
        "title": { "type": "keyword" },
        "body": { "type": "string" },
        "tags": { "type": "text" },
        "published_on": { "type": "text" },
        "comments": {
          "type": "nested",
          "properties": {
            "name":    { "type": "keyword"  },
            "comment": { "type": "string"  },
            "age":     { "type": "short"   },
            "rating":   { "type": "short"   },
            "commented_on":    { "type": "text"    }
          }
        }
      }
    }
  }
}

✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.
ℹ️ Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
wTdmHPQwWxXMGm*riZVF
✅ Both the ElasticSearch server user&password and the Kibana token should be displayed on the terminal outputs. SAVE THEM!

https://bitlaunch.io/blog/install-elasticsearch-on-ubuntu-20-04-lts/





Install and Configure Elasticsearch
--------------------------------------
In this step, we will install and configure the Elasticsearch on the single node server 'ELK20' with the internal IP address '172.16.0.3'.
sudo vim /etc/hosts
................
172.16.0.3    ELK20


sudo vim elasticsearch.yml
------------------------------
node.name: ELK20
network.host: 172.16.0.3
http.port: 9200
cluster.initial_master_nodes: ["ELK20"]
xpack.security.enabled: true

edit the JVM options to set the memory limits:
-------------------------------------------------------
$ sudo vi /etc/elasticsearch/jvm.options
-Xms1g
-Xmx1g
OR
echo -e "-Xms512M\n-Xmx512M" > /etc/elasticsearch/jvm.options.d/jvm.options

Next, we will generate the password for the built-in user on Elasticsearch.(all users)
-----------------------------------------------------------------------------
cd /usr/share/elasticsearch/
bin/elasticsearch-setup-passwords auto -u "http://172.16.0.3:9200"
Type 'y' to confirm and generate the password.

To test our Elasticsearch installation, run the curl command with the default user 'elastic' as below.
-----------------------------------------------------------------
curl -X GET -u elastic "http://172.16.0.3:9200/?pretty"                         OR   curl -k -XGET https://localhost:9200 -u elastic --cacert /etc/elasticsearch/certs/http_ca.crt
Type the password for 'elastic' user, and below is the result you will get.


sudo apt update -y && apt upgrade -y
sudo apt install kibana=elastic_version_number
sudo cp /etc/kibana/kibana.yml /etc/kibana/kibana.yml.bak
sudo vim /etc/kibana/kibana.yml
----------------------------------It's recommended to run Kibana on the local network because we will use the Nginx as a reverse proxy for Kibana.
server.port: 5601
server.host: "172.16.0.3"
server.name: "ELK20"
elasticsearch.url: "http://172.16.0.3:9200"
elasticsearch.username: "kibana_system"
elasticsearch.password: "N88VBkkelfSV3mBfO6Vh"
//server.host: '0.0.0.0'  //Uncomment and configure server.host to allow connection from remote hos
//elasticsearch.hosts: ["http://127.0.0.1:9200"]  //Uncomment elasticsearch.hosts and specify Elasticsearch server to connect to

Save the file and configure Kibana to run at startup and start its service.
-------------------------------
sudo systemctl daemon-reload
sudo systemctl enable kibana.service
sudo systemctl start kibana.service

https://www.howtoforge.com/tutorial/ubuntu-elastic-stack/
https://sergiiblog.com/java-elasticsearch/

Next, we will create a new user that will be used to log in to the Kibana dashboard.
Create a new user named 'hakase' and the password 'hakasepasskibana' with the role 'kibana_admin' as below.
curl -X POST -u elastic "http://172.16.0.3:9200/_security/user/hakase?pretty" -H 'Content-Type: application/json' -d'
{
  "password" : "hakasepasskibana",
  "roles" : [ "kibana_admin" ]
}
'





root@ubuntu20:~# systemctl start elasticsearch
.if service was unable to start, then check below log file for any errors.
-------------------------------------------------------------------------------
root@ubuntu20:~# tail -f /var/log/elasticsearch/elasticsearch.log













sudo vim /etc/elasticsearch/elasticsearch.yml
------------------------------------------------
network.host: 0.0.0.0
discovery.seed_hosts: [ ]
xpack.security.enabled: false

After changing in configuration file you need to restart so run the below command:
------------------------------------------
sudo systemctl restart elasticsearch



-> Elasticsearch security features have been automatically configured!
-> Authentication is enabled and cluster connections are encrypted.

->  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  odEvwWHdBK3yN+4EdIpU  == ayushman

->  HTTP CA certificate SHA-256 fingerprint:
  84490b5a69d6e31cf3b74b404b41b454567a51150d0ed3f91535312064663dd0

->  Configure Kibana to use this cluster: 
* Run Kibana and click the configuration link in the terminal when Kibana starts.
* Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjMuMyIsImFkciI6WyIxOTIuMTY4LjEuMjg6OTIwMCJdLCJmZ3IiOiI4NDQ5MGI1YTY5ZDZlMzFjZjNiNzRiNDA0YjQxYjQ1NDU2N2E1MTE1MGQwZWQzZjkxNTM1MzEyMDY0NjYzZGQwIiwia2V5IjoicW44UjZvWUJKcVM2c2tWMngxd3c6akZHYU9NaGFUUk9xUWp3Y21aLVZqZyJ9

bin/elasticsearch-create-enrollment-token -s kibana
new = eyJ2ZXIiOiI4LjMuMyIsImFkciI6WyIxOTIuMTY4LjEuMjg6OTIwMCJdLCJmZ3IiOiI4NDQ5MGI1YTY5ZDZlMzFjZjNiNzRiNDA0YjQxYjQ1NDU2N2E1MTE1MGQwZWQzZjkxNTM1MzEyMDY0NjYzZGQwIiwia2V5IjoiUFppcjZvWUIzbWVFSWw2N2pwdjM6aFZYY2lzZGdTZHlMOEtlRjNjU0hyQSJ9

->  Configure other nodes to join this cluster:
* On this node:
  - Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  - Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  - Restart Elasticsearch.
* On other nodes:
  - Start Elasticsearch with `bin/elasticsearch --enrollment-token <token>`, using the enrollment token that you generated.





Set user and password in Kibana.yml
---------------------------------------
#elasticsearch.username: "kibana_system"  //Change before
elasticsearch.username: "elastic"         //After change
#elasticsearch.password: "pass"           //Change before
elasticsearch.password: "*********"       //After change



install elk 8.x ===== 
------------------------------
https://www.securitynik.com/2022/04/installing-configuring-elasticsearch-8.html


imp setup certificate
--------------------------------------
https://techexpert.tips/elasticsearch/elasticsearch-enable-tls-https/
https://xie.infoq.cn/article/3f6ba28bddad16a92f8101de0
https://levelup.gitconnected.com/elasticsearch-kibana-security-authentication-and-network-encryption-enabled-with-docker-e6747a989766
https://quachtd.com/configure-https-for-elasticsearch/
https://alexmarquardt.com/tag/kibana/    ===============          https://blog.clairvoyantsoft.com/secure-elasticsearch-communication-bdee1bbe37e
https://dotblogs.com.tw/Lin/2022/10/18/191449
https://www.rondochen.com/ELK9/




bin/elasticsearch-certutil ca
ENTER ENTER
bin/elasticsearch-certutil cert --ca elastic-stack-ca.p12
ENTER ENTER ENTER
--------The newly created certificates should be copied into a sub-directory called certs located within the config directory. ----------
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: certs/elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: certs/elastic-certificates.p12


xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate
xpack.security.transport.ssl.keystore.path: elastic-certificates.p12
xpack.security.transport.ssl.truststore.path: elastic-certificates.p12

xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.keystore.path: http.p12









bin/elasticsearch-certutil http







@Document(indexName = "blog", type = "article")
public class Article {

    @Id
    private String id;
    
    private String title;
    
    @Field(type = FieldType.Nested, includeInParent = true)
    private List<Author> authors;
    
    // standard getters and setters
}




















@Configuration
public class ElasticsearchConfig {
    @Value("${elasticsearch.host}")
    private String host;

    @Value("${elasticsearch.port}")
    private int port;

    @Value("${elasticsearch.username}")
    private String userName;

    @Value("${elasticsearch.password}")
    private String password;

    @Bean(destroyMethod = "close")
    public RestHighLevelClient restClient() {

        final CredentialsProvider credentialsProvider = new BasicCredentialsProvider();
        credentialsProvider.setCredentials(AuthScope.ANY,
                new UsernamePasswordCredentials(userName, password));

        RestClientBuilder builder = RestClient.builder(new HttpHost(host, port))
                .setHttpClientConfigCallback(httpClientBuilder -> httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider));

        RestHighLevelClient client = new RestHighLevelClient(builder);

        return client;
    }
}

@Data
public class Technologies {
    private String name;
    private  String yearsOfExperience;
}


@Data
public class Location {
    private String lat;
    private String lon;
}


@Data
public class ProfileDocument {
    private String id;
    private String firstName; text
    private String lastName; text
    private List<Technologies> technologies; //@Field(type=FieldType.Nested)
    private Location location;
    private List<String> emails;
}

https://github.com/spati-java/spring-boot-java-highlevel-rest-client-elasticsearch/blob/develop/src/main/java/com/example/aws/elasticsearch/demo/service/ProfileService.java
@Service
@Slf4j
public class ProfileService {


    private RestHighLevelClient client;


    private ObjectMapper objectMapper;

    @Autowired
    public ProfileService(RestHighLevelClient client, ObjectMapper objectMapper) {
        this.client = client;
        this.objectMapper = objectMapper;
    }

    public String createProfileDocument(ProfileDocument document) throws Exception {

        UUID uuid = UUID.randomUUID();
        document.setId(uuid.toString());

        IndexRequest indexRequest = new IndexRequest(INDEX, TYPE, document.getId())
                .source(convertProfileDocumentToMap(document), XContentType.JSON);

        IndexResponse indexResponse = client.index(indexRequest, RequestOptions.DEFAULT);
        return indexResponse.getResult().name();
    }

    public ProfileDocument findById(String id) throws Exception {

            GetRequest getRequest = new GetRequest(INDEX, TYPE, id);

            GetResponse getResponse = client.get(getRequest, RequestOptions.DEFAULT);
            Map<String, Object> resultMap = getResponse.getSource();

            return convertMapToProfileDocument(resultMap);

    }



    public String updateProfile(ProfileDocument document) throws Exception {

            ProfileDocument resultDocument = findById(document.getId());

            UpdateRequest updateRequest = new UpdateRequest(
                    INDEX,
                    TYPE,
                    resultDocument.getId());

            updateRequest.doc(convertProfileDocumentToMap(document));
            UpdateResponse updateResponse = client.update(updateRequest, RequestOptions.DEFAULT);

            return updateResponse
                    .getResult()
                    .name();

    }

    public List<ProfileDocument> findAll() throws Exception {


        SearchRequest searchRequest = buildSearchRequest(INDEX,TYPE);
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();
        searchSourceBuilder.query(QueryBuilders.matchAllQuery());
        searchRequest.source(searchSourceBuilder);

        SearchResponse searchResponse =
                client.search(searchRequest, RequestOptions.DEFAULT);

        return getSearchResult(searchResponse);
    }


    public List<ProfileDocument> findProfileByName(String name) throws Exception{


        SearchRequest searchRequest = new SearchRequest();
        searchRequest.indices(INDEX);
        searchRequest.types(TYPE);

        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        MatchQueryBuilder matchQueryBuilder = QueryBuilders
                .matchQuery("name",name)
                .operator(Operator.AND);

        searchSourceBuilder.query(matchQueryBuilder);

        searchRequest.source(searchSourceBuilder);

        SearchResponse searchResponse =
                client.search(searchRequest, RequestOptions.DEFAULT);

        return getSearchResult(searchResponse);

    }


    public String deleteProfileDocument(String id) throws Exception {

        DeleteRequest deleteRequest = new DeleteRequest(INDEX, TYPE, id);
        DeleteResponse response = client.delete(deleteRequest,RequestOptions.DEFAULT);

        return response
                .getResult()
                .name();

    }

    private Map<String, Object> convertProfileDocumentToMap(ProfileDocument profileDocument) {
        return objectMapper.convertValue(profileDocument, Map.class);
    }

    private ProfileDocument convertMapToProfileDocument(Map<String, Object> map){
        return objectMapper.convertValue(map,ProfileDocument.class);
    }


    public List<ProfileDocument> searchByTechnology(String technology) throws Exception{

        SearchRequest searchRequest = buildSearchRequest(INDEX,TYPE);
        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();

        QueryBuilder queryBuilder = QueryBuilders
                .boolQuery()
                .must(QueryBuilders
                        .matchQuery("technologies.name",technology));

        searchSourceBuilder.query(QueryBuilders.nestedQuery("technologies",queryBuilder,ScoreMode.Avg));

        searchRequest.source(searchSourceBuilder);

        SearchResponse response = client.search(searchRequest,RequestOptions.DEFAULT);

        return getSearchResult(response);
    }

    private List<ProfileDocument> getSearchResult(SearchResponse response) {

        SearchHit[] searchHit = response.getHits().getHits();

        List<ProfileDocument> profileDocuments = new ArrayList<>();

        for (SearchHit hit : searchHit){
            profileDocuments
                    .add(objectMapper
                            .convertValue(hit
                                    .getSourceAsMap(), ProfileDocument.class));
        }

        return profileDocuments;
    }

    private SearchRequest buildSearchRequest(String index, String type) {

        SearchRequest searchRequest = new SearchRequest();
        searchRequest.indices(index);
        searchRequest.types(type);

        return searchRequest;
    }
}


Parameters such as the number of shards and refresh time are no longer declared ​@Docement​in , indicating that they have been deprecated Instead, declare ​@Setting ​in
@Data 
@Document(indexName = "user") 
@Setting(replicas = 0,shards = 1) 
public class User implements Serializable { 

    @Id 
    private String id ; 

    @Field(type = FieldType.Text, name = "name",analyzer = "standard") 
    private String name; 

    @Field(type = FieldType.Keyword, name = "password") 
    private String password; 

    @Field(type = FieldType.Integer, name = "sex") 
    private Integer sex; 

    @Field(type = FieldType.Text, name = "address", analyzer = "ik_smart") 
    private String address;

    @Field(type = FieldType.Date, name = "create_date", pattern="yyyy-MM-dd HH:mm:ss||strict_date_optional_time||epoch_millis") 
    private Date createDate; 
    
    @Field(type = FieldType.Long , name = "role_ids") 
    private List<Long> roleIds; 

    @Field(type = FieldType.Nested, name = "department_list") 
    private List<Department> departmentList; 

} 

@Data 
public class Department implements Serializable { 

    @Field(type = FieldType. Long, name = "id") 
    private Long id; 

    @Field(type = FieldType. Keyword, name = "name") 
    private String name; 
}
-----------------------------------


 
@Document(indexName="productindex")
@Data
public class Product {
 
    @Id
    private String id;
 
    @Field(type= FieldType.Text, name="productName")
    private String productName;
 
    @Field(type= FieldType.Text, name="productDescription")
    private String productDescription;
 
    @Field(type= FieldType.Double, name="productPrice")
    private BigDecimal productPrice;
 
    @Field(type= FieldType.Integer, name="quantity")
    private Integer quantity;
 
    @Field(type= FieldType.Text, name="sportsCategory")
    private String sportsCategory;
 
    @Field(type= FieldType.Text, name="manufacturer")
    private String manufacturer;
}


   @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss.SSS")
    @Field(type= FieldType.Date, format= DateFormat.custom, pattern="yyyy-MM-dd HH:mm:ss.SSS")
    private Date createTime;

    @JsonFormat(pattern = "yyyy-MM-dd HH:mm:ss.SSS")
    @Field(type=FieldType.Date, format=DateFormat.custom, pattern="yyyy-MM-dd HH:mm:ss.SSS")
    private Date updateTime;
-----------------------------------
   @Field(type= FieldType.Nested, index = FieldIndex.not_analyzed)
    private List<Stock> relatedStocks;
    @Field(type= FieldType.String, index = FieldIndex.not_analyzed)
    private List<String> relatedIndustries;
    
    @JsonFormat(shape = JsonFormat.Shape.STRING, pattern = "yyyyMMdd'T'HHmmss.SSS'Z'")
    @Field(type = FieldType.Date, format = DateFormat.basic_date_time, index = FieldIndex.not_analyzed)
    @CreatedDate
    private Date createdDateTime;




https://juejin.cn/post/7120587835358314504
@Data
@Document(indexName = "order_test")
@Setting(replicas = 0)
public class Order {

    @Id
    private String id;

    // 订单状态 0未付款 1未发货 2运输中 3待签收 4已签收
    @Field(type = FieldType.Integer, name = "status")
    private Integer status;

    @Field(type = FieldType.Keyword, name = "no")
    private String no;

    @Field(type = FieldType.Date, name = "create_time", pattern = "yyyy-MM-dd HH:mm:ss")
    private Date createTime;

    @Field(type = FieldType.Double, name = "amount")
    private Double amount;

    @Field(type = FieldType.Keyword, name = "creator")
    private String creator;

    @GeoPointField
    @Field(name = "point")
    private GeoPoint point;

    @Field(type = FieldType.Text, name = "address", analyzer = "ik_max_word")//@Field(type = FieldType.Text, name = "address", analyzer = "ik_smart") 
    private String address;
    
    @Field(type = FieldType.Nested, name = "creator")
    private List<Product> product;

}

@Data
public class Product implements Serializable {

    @Field(type = FieldType.Long, name = "id")
    private Long id;

    @Field(type = FieldType.Keyword, name = "name")
    private String name;

    @Field(type = FieldType.Double, name = "price")
    private Double price;

    @Field(type = FieldType.Integer, name = "quantity")
    private Double quantity;

}






@Data
@AllArgsConstructor
@NoArgsConstructor
@Builder
@Document(indexName = "products", type = "managedproducts", shards = 1, replicas = 0, refreshInterval = "5s", createIndex = true)
//@Document(indexName = "person" , type = "user")
//@Setting(shards = 1, replicas = 0)
public class Product {
    @Id
    @Field(type = FieldType.Keyword, store = true)
    private String id = null;

    @Field(type = FieldType.Text)
    private String name = null ;

    @Field(type = FieldType.Keyword)
    private Category category;

    @Field(type = FieldType.Integer)
    private Integer quantity;

    @Field(type = FieldType.Double)
    private BigDecimal price;

    @Field(type = FieldType.Long)
    private double price;

    public enum Category {
        CLOTHES,
        ELECTRONICS,
        GAMES;
    }

    @Field(type = FieldType.Text, name = "address", analyzer = "ik_max_word")
    private String address;

    @Field(type=FieldType.Boolean)
    private Boolean active = null;

    @Field(type = FieldType.Date, format = DateFormat.date_time)
    private Instant createdDate = null;

    @Field(type = FieldType.Date, name = "create_time", pattern = "yyyy-MM-dd HH:mm:ss")
    private Date createTime;

    @Field(type = FieldType.Double, name = "price")
    private double price;

    @Field(analyzer = "ik_max_word", searchAnalyzer = "ik_smart", type = FieldType.Text, store = true)
    private String spuName;

    @Field(type = FieldType.Long)
    private long sCateId;

    @Field(type = FieldType.Date, format = DateFormat.custom, pattern = "yyyy-MM-dd HH:mm:ss")
    @JsonSerialize(using = CustomLocalDateTimeSerializer.class)
    @JsonDeserialize(using = CustomLocalDateTimeDeserializer.class)
    private LocalDateTime createTime = LocalDateTime.now();
}


{
  "hotel": {
    "hotel_id": 1,
    "email": "hotel-1@trvb.com",
    "city_name_en": "Warsaw",
    "name": "Golden star hotel",
    "stars": 5,
    "rating": 4.85,
    "age": 7,
    "free_places_at_now": true,
    "location": {
      "lat": "52.21",
      "lon": "21.01"
    },
    "comments": [
      {
        "hotel_id": 1,
        "content": "Some comment",
        "stars": 5,
        "created_at": "2021/08/01"
      },
      {
        "hotel_id": 1,
        "content": "Some comment",
        "created_at": "2021/08/01",
        "stars": 5
      }
    ]
  }
}









@Data
@Document(indexName = "operation_log")
public class OperationLog {
    @Id
    private String id;

    @Field(type = FieldType.Keyword)
    private String ip;

    @Field(value = "trace_id", type = FieldType.Keyword)
    private String traceId;

    // format={} 不能少
    @Field(value = "operation_time", type = FieldType.Date, format = {}, pattern = "yyyy-MM-dd HH:mm:ss")
    @JsonFormat(pattern = "yyyy.MM.dd HH:mm:ss", timezone = "GMT+8")
    private LocalDateTime operationTime;

    @Field(type = FieldType.Keyword)
    private String module;

    @Field(value = "action_code", type = FieldType.Keyword)
    private String actionCode;

    @Field(type = FieldType.Text, analyzer = "ik_max_word")
    private String location;

    @Field(value = "object_id", type = FieldType.Keyword)
    private String objectId;

    @Field(value = "object_name", type = FieldType.Text, analyzer = "ik_max_word")
    private String objectName;

    @Field(value = "operator_id", type = FieldType.Keyword)
    private String operatorId;

    @Field(value = "operator_name", type = FieldType.Keyword)
    private String operatorName;

    @Field(value = "operator_dept_id", type = FieldType.Keyword)
    private String operatorDeptId;

    @Field(value = "operator_dept_name", type = FieldType.Text, analyzer = "ik_max_word")
    private String operatorDeptName;

    @Field(type = FieldType.Nested)
    private List<OperationLogChange> changes;

}

@Data
public class OperationLogChange {
    @Field(value = "field_name", type = FieldType.Keyword)
    private String fieldName;

    @Field(value = "old_value", type = FieldType.Keyword)
    private String oldValue;

    @Field(value = "new_value", type = FieldType.Keyword)
    private String newValue;
}

https://segmentfault.com/a/1190000042672215
https://segmentfault.com/a/1190000042853722
PUT /operation_log/_mapping
{
  "properties": {
    "ip": {
      "type": "keyword"
    },
    "trace_id": {
      "type": "keyword"
    },
    "operation_time": {
      "type": "date",
      "format": "yyyy-MM-dd HH:mm:ss"
    },
    "module": {
      "type": "keyword"
    },
    "action_code": {
      "type": "keyword"
    },
    "location": {
      "type": "text",
      "analyzer": "ik_max_word",
      "fields": {
        "keyword": {
          "type": "keyword"
        }
      }
    },
    "object_id": {
      "type": "keyword"
    },
    "object_name": {
      "type": "text",
      "analyzer": "ik_max_word",
      "fields": {
        "keyword": {
          "type": "keyword"
        }
      }
    },
    "operator_id": {
      "type": "keyword"
    },
    "operator_name": {
      "type": "keyword"
    },
    "operator_dept_id": {
      "type": "keyword"
    },
    "operator_dept_name": {
      "type": "text",
      "analyzer": "ik_max_word",
      "fields": {
        "keyword": {
          "type": "keyword"
        }
      }
    },
    "changes": {
      "type": "nested",
      "properties": {
        "field_name": {
          "type": "keyword"
        },
        "old_value": {
          "type": "keyword"
        },
        "new_value": {
          "type": "keyword"
        }
      }
    }
  }
}


POST /operation_log/_doc
{
  "ip": "10.1.11.1",
  "trace_id": "670021ff9a2dc6b7",
  "operation_time": "2022-05-02 09:31:18",
  "module": "企业组织",
  "action_code": "UPDATE",
  "location": "企业组织->员工管理->身份管理",
  "object_id": "xxxxx-1",
  "object_name": "成德善",
  "operator_id": "operator_id-1",
  "operator_name": "张三",
  "operator_dept_id": "operator_dept_id-1",
  "operator_dept_name": "研发中心-后端一部",
  "changes": [
    {
      "field_name": "手机号码",
      "old_value": "13055660000",
      "new_value": "13055770001"
    },
    {
      "field_name": "姓名",
      "old_value": "成德善",
      "new_value": "成秀妍"
    }
  ]
}

// 同样的调用方式，再插入下面6个文档

// data-2

{
  "ip": "22.1.11.0",
  "trace_id": "990821e89a2dc653",
  "operation_time": "2022-09-05 11:31:10",
  "module": "资源中心",
  "action_code": "UPDATE",
  "location": "资源中心->文件管理->文件权限",
  "object_id": "fffff-1",
  "object_name": "《2022员工绩效打分细则》",
  "operator_id": "operator_id-2",
  "operator_name": "李四",
  "operator_dept_id": "operator_dept_id-2",
  "operator_dept_name": "人力资源部",
  "changes": [
    {
      "field_name": "查看权限",
      "old_value": "仅李四可查看",
      "new_value": "全员可查看"
    },
    {
      "field_name": "编辑权限",
      "old_value": "仅李四可查看",
      "new_value": "人力资源部可查看"
    }
  ]
}

// data-3

{
  "ip": "22.1.11.0",
  "trace_id": "780821e89b2dc653",
  "operation_time": "2022-10-02 12:31:10",
  "module": "资源中心",
  "action_code": "DELETE",
  "location": "资源中心->文件管理",
  "object_id": "fffff-1",
  "object_name": "《2022员工绩效打分细则》",
  "operator_id": "operator_id-3",
  "operator_name": "王五",
  "operator_dept_id": "operator_dept_id-2",
  "operator_dept_name": "人力资源部",
  "changes": []
}

// data-4

{
  "ip": "10.1.11.1",
  "trace_id": "670021e89a2dc7b6",
  "operation_time": "2022-05-03 09:35:10",
  "module": "企业组织",
  "action_code": "ADD",
  "location": "企业组织->员工管理->身份管理",
  "object_id": "xxxxx-2",
  "object_name": "成宝拉",
  "operator_id": "operator_id-1",
  "operator_name": "张三",
  "operator_dept_id": "operator_dept_id-1",
  "operator_dept_name": "研发中心-后端一部",
  "changes": [
    {
      "field_name": "姓名",
      "new_value": "成宝拉"
    },
    {
      "field_name": "性别",
      "new_value": "女"
    },
    {
      "field_name": "手机号码",
      "new_value": "13055770002"
    },
    {
      "field_name": "邮箱",
      "new_value": "baola@qq.com"
    }
  ]
}

// data-5

{
  "ip": "10.1.11.5",
  "trace_id": "670021e89a2dc655",
  "operation_time": "2022-05-05 10:35:12",
  "module": "企业组织",
  "action_code": "DELETE",
  "location": "企业组织->员工管理->身份管理",
  "object_id": "xxxxx-1",
  "object_name": "成德善",
  "operator_id": "operator_id-2",
  "operator_name": "李四",
  "operator_dept_id": "operator_dept_id-2",
  "operator_dept_name": "人力资源部",
  "changes": []
}

// data-6

{
  "ip": "10.0.0.0",
  "trace_id": "670021ff9a28ei6",
  "operation_time": "2022-10-02 09:31:00",
  "module": "资源中心",
  "action_code": "DELETE",
  "location": "资源中心->文件管理",
  "object_id": "fffff-a",
  "object_name": "《有空字符串的文档》",
  "operator_id": "operator_id-a",
  "operator_dept_id": "",
  "operator_dept_name": "",
  "operator_name": "路人A",
  "changes": []
}

// data-7

{
  "ip": "10.0.0.0",
  "trace_id": "670021ff9a28768",
  "operation_time": "2022-10-02 09:32:00",
  "module": "资源中心",
  "action_code": "DELETE",
  "location": "资源中心->文件管理",
  "object_id": "fffff-b",
  "object_name": "《有NULL的文档》",
  "operator_id": "operator_id-b",
  "operator_name": "路人B",
  "changes": []
}




    @Field(type= FieldType.Nested, index = FieldIndex.not_analyzed)
    private List<Stock> relatedStocks;
    @Field(type= FieldType.String, index = FieldIndex.not_analyzed)
    private List<String> relatedIndustries;


a.	SFTP_CONFIG(sftp_config)sftpConfig
====================
2.	partnerId
3.	ownerType(SftpConfigOwnerType) => PARTNER
4.	ownerId =>Owner ID of Partner
5.	sftpAlias
6.	sftpPath
7.	portNo
8.	username
9.	password
10.	uploadAttempts
11.	remarks
12.	status(SftpConfigStatus) => ACTIVE,INACTIVE
13.	Created Date
14.	Last Modified Date


b.	SFTP_FILE_UPLOAD_LIST(sftpFileUploadList)
==================================
2.	partnerId   M2O Partner
6.	sftpConfigId => Reference to SFTP Config   M2O  SftpConfig

3.	ownerType
4.	ownerId
5.	filePath
7.	sftpPath => to be populated from SFTP Config ID
8.	portNo => to be populated from SFTP Config ID
9.	sftpUploadStatus     SftpFileUploadStatus	SUCCESS,FAIL,PENDING
10.	sftpUploadMessage
11.	sftpUploadAttempts
12.	Created Date
13.	Last Modified Date


sftpDetails

